SEED = 42

import random, os
random.seed(SEED)

import numpy as np
np.random.seed(SEED)

import torch
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)        

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from copy import deepcopy
import tensorflow as tf
from sklearn.metrics import accuracy_score, mean_absolute_error
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import scipy.io
from __future__ import annotations
import math
from typing import Sequence
from torch import nn, Tensor
from torch.nn import functional as F
import scipy.io
from scipy.integrate import odeint

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Utility functions
def train_test_divide(ori_data, generated_data, ori_time, generated_time, train_rate=0.8):
    """Divide train and test data for both original and generated data."""
    no = len(ori_data)
    idx = np.random.permutation(no)
    train_idx = idx[:int(no*train_rate)]
    test_idx = idx[int(no*train_rate):]
    
    train_x = [ori_data[i] for i in train_idx]
    test_x = [ori_data[i] for i in test_idx]
    train_t = [ori_time[i] for i in train_idx]
    test_t = [ori_time[i] for i in test_idx]
    
    # Divide train/test index (generated data)
    no = len(generated_data)
    idx = np.random.permutation(no)
    train_idx = idx[:int(no*train_rate)]
    test_idx = idx[int(no*train_rate):]
    
    train_x_hat = [generated_data[i] for i in train_idx]
    test_x_hat = [generated_data[i] for i in test_idx]
    train_t_hat = [generated_time[i] for i in train_idx]
    test_t_hat = [generated_time[i] for i in test_idx]
    
    return train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat

def extract_time(data):
    """Returns Maximum sequence length and each sequence length."""
    time = list()
    max_seq_len = 0
    for i in range(len(data)):
        max_seq_len = max(max_seq_len, len(data[i][:,0]))
        time.append(len(data[i][:,0]))
    return time, max_seq_len

def batch_generator(data, time, batch_size):
    """Mini-batch generator."""
    no = len(data)
    idx = np.random.permutation(no)
    train_idx = idx[:batch_size]     
    X_mb = list(data[i] for i in train_idx)
    T_mb = list(time[i] for i in train_idx)
    return X_mb, T_mb

def MinMaxScaler(data):
  """Min-Max Normalizer.
  
  Args:
    - data: raw data
    
  Returns:
    - norm_data: normalized data
    - min_val: minimum values (for renormalization)
    - max_val: maximum values (for renormalization)
  """    
  min_val = np.min(np.min(data, axis = 0), axis = 0)
  data = data - min_val
    
  max_val = np.max(np.max(data, axis = 0), axis = 0)
  norm_data = data / (max_val + 1e-7)
    
  return norm_data

def visualization (ori_data, generated_data, analysis, name = 'tsne'):
  """Using PCA or tSNE for generated and original data visualization.
  
  Args:
    - ori_data: original data
    - generated_data: generated synthetic data
    - analysis: tsne or pca
  """  
  # Analysis sample size (for faster computation)
  anal_sample_no = min([1000, len(ori_data)])
  idx = np.random.permutation(len(ori_data))[:anal_sample_no]
    
  # Data preprocessing
  ori_data = np.asarray(ori_data)
  generated_data = np.asarray(generated_data)  
  
  ori_data = ori_data[idx]
  generated_data = generated_data[idx]
  
  no, seq_len, dim = ori_data.shape  
  
  for i in range(anal_sample_no):
    if (i == 0):
      prep_data = np.reshape(np.mean(ori_data[0,:,:], 1), [1,seq_len])
      prep_data_hat = np.reshape(np.mean(generated_data[0,:,:],1), [1,seq_len])
    else:
      prep_data = np.concatenate((prep_data, 
                                  np.reshape(np.mean(ori_data[i,:,:],1), [1,seq_len])))
      prep_data_hat = np.concatenate((prep_data_hat, 
                                      np.reshape(np.mean(generated_data[i,:,:],1), [1,seq_len])))
    
  # Visualization parameter        
  colors = ["red" for i in range(anal_sample_no)] + ["blue" for i in range(anal_sample_no)]    
    
  if analysis == 'pca':
    # PCA Analysis
    pca = PCA(n_components = 2)
    pca.fit(prep_data)
    pca_results = pca.transform(prep_data)
    pca_hat_results = pca.transform(prep_data_hat)
    
    # Plotting
    f, ax = plt.subplots(1)    
    plt.scatter(pca_results[:,0], pca_results[:,1],
                c = colors[:anal_sample_no], alpha = 0.2)#, label = "Original"
    plt.scatter(pca_hat_results[:,0], pca_hat_results[:,1], 
                c = colors[anal_sample_no:], alpha = 0.2)#, label = "Synthetic"
  
    #ax.legend()  
    #plt.title('PCA plot', fontsize=21)
    # plt.xlabel('x-pca')
    # plt.ylabel('y_pca')
    plt.tick_params(labelsize=21)
    plt.savefig(name)
    plt.show()
    
  elif analysis == 'tsne':
    
    # Do t-SNE Analysis together       
    prep_data_final = np.concatenate((prep_data, prep_data_hat), axis = 0)
    
    # TSNE anlaysis
    tsne = TSNE(n_components = 2, verbose = 1, perplexity = 40, n_iter = 300)
    tsne_results = tsne.fit_transform(prep_data_final)
      
    # Plotting
    f, ax = plt.subplots(1)
      
    plt.scatter(tsne_results[:anal_sample_no,0], tsne_results[:anal_sample_no,1], 
                c = colors[:anal_sample_no], alpha = 0.2)#
    plt.scatter(tsne_results[anal_sample_no:,0], tsne_results[anal_sample_no:,1], 
                c = colors[anal_sample_no:], alpha = 0.2)#, label = "Synthetic"
  
    #ax.legend()
      
    #plt.title('t-SNE plot', fontsize=21)
    # plt.xlabel('x-tsne')
    # plt.ylabel('y_tsne')
    plt.tick_params(labelsize=21)
    plt.savefig(name)
    plt.show()    

# Model Definitions
class GMMPrior(nn.Module):
    """Learnable isotropic Gaussian mixture prior with equal weights."""
    def __init__(self, K: int, latent_dim: int):
        super().__init__()
        self.K = K
        self.latent_dim = latent_dim
        self.mu = nn.Parameter(torch.randn(K, latent_dim) * 0.05)
        self.logvar = nn.Parameter(torch.zeros(K, latent_dim))  # log ÏƒÂ²_k

    @property
    def var(self) -> Tensor:
        return self.logvar.exp()

    def forward(self) -> tuple[Tensor, Tensor]:
        return self.mu, self.var

def gaussian_overlap(mu1: Tensor, var1: Tensor, mu2: Tensor, var2: Tensor) -> Tensor:
    """Computes ð“(Î¼1 | Î¼2, Î£1 + Î£2) for diagonal covariances (isotropic per dim)."""
    var_sum = var1 + var2
    diff = mu1 - mu2
    
    # åœ¨å¯¹æ•°ç©ºé—´è®¡ç®—ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
    D = mu1.size(-1)
    log_norm = -0.5 * D * math.log(2 * math.pi) - 0.5 * var_sum.log().sum(dim=-1)
    log_exp = -0.5 * (diff.pow(2) / var_sum).sum(dim=-1)
    
    return (log_norm + log_exp).exp()

def cs_divergence_gmm(mu_q: Tensor, var_q: Tensor, mu_p: Tensor, var_p: Tensor) -> Tensor:
    """Compute D_CS(q||p) for Gaussian q vs GMM p with equal weights."""
    K = mu_p.size(0)
    D = mu_q.size(-1)
    B = mu_q.size(0)
    
    # Term 1: âˆ« q(z)p(z)dz = 1/K âˆ‘_k N(Î¼_q | Î¼_k, Ïƒ_qÂ² + Ïƒ_kÂ²)
    overlap_qp = gaussian_overlap(
        mu_q.unsqueeze(1), var_q.unsqueeze(1), 
        mu_p.unsqueeze(0), var_p.unsqueeze(0)
    )  # (B, K)
    term1 = overlap_qp.mean(dim=1)  # (B,)
    
    # Term 2: âˆ« p(z)Â²dz = 1/KÂ² âˆ‘_{k,k'} N(Î¼_k | Î¼_k', 2Ïƒ_k'Â²)
    overlap_pp = gaussian_overlap(
        mu_p.unsqueeze(1), var_p.unsqueeze(1),  # (K, 1, D)
        mu_p.unsqueeze(0), var_p.unsqueeze(0)   # (1, K, D)
    )  #  (K, K)
    term2 = overlap_pp.mean()
    
    # Term 3: âˆ« q(z)Â²dz = N(Î¼_q | Î¼_q, 2Ïƒ_qÂ²)
    log_term3 = -0.5 * D * math.log(2 * math.pi) - 0.5 * (2 * var_q).log().sum(dim=-1)
    term3 = log_term3.exp()  # (B,)
    
    # D_CS = -log(term1) + 0.5*log(term2) + 0.5*log(term3)
    cs_div = -term1.log() + 0.5 * term2.log() + 0.5 * term3.log()
    
    return cs_div.clamp(min=0)


class GRU(nn.Module):
    def __init__(self, num_series, hidden):
        super(GRU, self).__init__()
        self.p = num_series
        self.hidden = hidden

        # Set up network.
        self.gru = nn.GRU(num_series, hidden, batch_first=True)
        self.gru.flatten_parameters()
        self.linear = nn.Linear(hidden, 1)
        self.sigmoid = nn.Sigmoid()

    def init_hidden(self, batch):
        #Initialize hidden states
        device = self.gru.weight_ih_l0.device
        return torch.zeros(1, batch, self.hidden, device=device)
               
    def forward(self, X, z, connection, mode = 'train'):
        X=X[:,:,np.where(connection!=0)[0]]
        device = self.gru.weight_ih_l0.device
        tau = 0
        if mode == 'train':
          X_right, hidden_out = self.gru(torch.cat((X[:,0:1,:],X[:,11:-1,:]),1), z)
          X_right = self.linear(X_right)
          return X_right, hidden_out

class VRAE4E(nn.Module):
    def __init__(self, num_series, hidden, K=10, lambda_cs=0.1):
       
        super(VRAE4E, self).__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.p = num_series
        self.hidden = hidden
        self.lambda_cs = lambda_cs
        
        self.gru_left = nn.GRU(num_series, hidden, batch_first=True)
        self.gru_left.flatten_parameters()
        
        self.fc_mu = nn.Linear(hidden, hidden)
        self.fc_std = nn.Linear(hidden, hidden)
        
        self.linear_hidden = nn.Linear(hidden, hidden)
        self.tanh = nn.Tanh()
        
        self.gru = nn.GRU(num_series, hidden, batch_first=True)
        self.gru.flatten_parameters()
        self.linear = nn.Linear(hidden, num_series)
        
        if K > 1:
            self.prior = GMMPrior(K, hidden)
            self.use_gmm = True
        else:
            self.register_buffer('prior_mu', torch.zeros(1, hidden))
            self.register_buffer('prior_var', torch.ones(1, hidden))
            self.use_gmm = False

    def init_hidden(self, batch):
        '''Initialize hidden states for GRU cell.'''
        device = self.gru.weight_ih_l0.device
        return torch.zeros(1, batch, self.hidden, device=device)
               
    def forward(self, X, mode = 'train'):
        X = torch.cat((torch.zeros(X.shape,device = self.device)[:,0:1,:],X),1)
        if mode == 'train':
            hidden_0 = torch.zeros(1, X.shape[0], self.hidden, device=self.device)
            out, h_t = self.gru_left(X[:,1:,:], hidden_0.detach())
            
            mu = self.fc_mu(h_t)
            log_var = self.fc_std(h_t)
            
            sigma = torch.exp(0.5*log_var)
            z = torch.randn(size = mu.size())
            z = z.type_as(mu) 
            z = mu + sigma*z
            z = self.tanh(self.linear_hidden(z))
            
            X_right, hidden_out = self.gru(X[:,:-1,:], z)
            pred = self.linear(X_right)
            
            return pred, mu, log_var
            
        if mode == 'test':
            X_seq = torch.zeros(X[:,:1,:].shape).to(self.device)
            h_t = torch.randn(size = (1, X_seq[:,-2:-1,:].size(0),self.hidden)).to(self.device)
            for i in range(int(20/1)+1):
                out, h_t = self.gru(X_seq[:,-1:,:], h_t)
                out = self.linear(out)
                X_seq = torch.cat([X_seq,out],dim = 1)
            return X_seq

class CRVAE(nn.Module):
    def __init__(self, num_series, connection, hidden, K, lambda_cs):
        '''
        connection: pruned networks
        '''
        super(CRVAE, self).__init__()
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.p = num_series
        self.hidden = hidden
        
        self.gru_left = nn.GRU(num_series, hidden, batch_first=True)
        self.gru_left.flatten_parameters()
        
        self.fc_mu = nn.Linear(hidden, hidden)
        self.fc_std = nn.Linear(hidden, hidden)
        self.connection = connection
        
        self.lambda_cs = lambda_cs
        self.prior = GMMPrior(K, hidden)
        # Set up networks.
        self.networks = nn.ModuleList([
            GRU(int(connection[:,i].sum()), hidden) for i in range(num_series)])

    def forward(self, X, noise = None, mode = 'train', phase = 0):
        if phase == 0:
            X = torch.cat((torch.zeros(X.shape,device = self.device)[:,0:1,:],X),1)
            if mode == 'train':
                hidden_0 = torch.zeros(1, X.shape[0], self.hidden, device=self.device)
                out, h_t = self.gru_left(X[:,1:11,:], hidden_0.detach())
                
                mu = self.fc_mu(h_t)
                log_var = self.fc_std(h_t)
                
                sigma = torch.exp(0.5*log_var)
                z = torch.randn(size = mu.size())
                z = z.type_as(mu)
                z = mu + sigma*z
    
                pred = [self.networks[i](X, z, self.connection[:,i])[0]
                      for i in range(self.p)]
    
                return pred, mu, log_var
                
            if mode == 'test':
                X_seq = torch.zeros(X[:,:1,:].shape).to(self.device)
                h_0 = torch.randn(size = (1, X_seq[:,-2:-1,:].size(0),self.hidden)).to(self.device)
                ht_last =[]
                for i in range(self.p):
                    ht_last.append(h_0)
                for i in range(int(20/1)+1):
                    ht_new = []
                    for j in range(self.p):
                        out, h_t = self.networks[j](X_seq[:,-1:,:], ht_last[j], self.connection[:,j])
                        if j == 0:
                            X_t = out
                        else:
                            X_t = torch.cat((X_t,out),-1)
                        ht_new.append(h_t)
                    ht_last = ht_new
                    if i ==0:
                        X_seq = X_t
                    else:
                        X_seq = torch.cat([X_seq,X_t],dim = 1)
                return X_seq
        
        if phase == 1:
            X = torch.cat((torch.zeros(X.shape,device = self.device)[:,0:1,:],X),1)
            if mode == 'train':
                hidden_0 = torch.zeros(1, X.shape[0], self.hidden, device=self.device)
                out, h_t = self.gru_left(X[:,1:11,:], hidden_0.detach())
                
                mu = self.fc_mu(h_t)
                log_var = self.fc_std(h_t)
                
                sigma = torch.exp(0.5*log_var)
                z = torch.randn(size = mu.size())
                z = z.type_as(mu)
                z = mu + sigma*z
    
                pred = [self.networks[i](X, z, self.connection[:,i])[0]
                      for i in range(self.p)]
    
                return pred, mu, log_var
                
            if mode == 'test':
                X_seq = torch.zeros(X[:,:1,:].shape).to(self.device)
                h_0 = torch.randn(size = (1, X_seq[:,-2:-1,:].size(0),self.hidden)).to(self.device)
                ht_last =[]
                for i in range(self.p):
                    ht_last.append(h_0)
                for i in range(int(20/1)+1):
                    ht_new = []
                    for j in range(self.p):
                        out, h_t = self.networks[j](X_seq[:,-1:,:], ht_last[j], self.connection[:,j])
                        if j == 0:
                            X_t = out
                        else:
                            X_t = torch.cat((X_t,out),-1)
                        ht_new.append(h_t)
                    ht_last = ht_new
                    if i ==0:
                        X_seq = X_t + 0.1*noise[:,i:i+1,:] 
                    else:
                        X_seq = torch.cat([X_seq,X_t+0.1*noise[:,i:i+1,:]],dim = 1)
                return X_seq

    def GC(self, threshold=True):
        '''Extract learned Granger causality.'''
        GC = [torch.norm(net.gru.weight_ih_l0, dim=0)
              for net in self.networks]
        GC = torch.stack(GC)
        if threshold:
            return (torch.abs(GC) > 0).int()
        else:
            return GC

# Training utility functions
def prox_update(network, lam, lr):
    '''Perform in place proximal update on first layer weight matrix.'''
    W = network.gru.weight_ih_l0
    norm = torch.norm(W, dim=0, keepdim=True)
    W.data = ((W / torch.clamp(norm, min=(lam * lr)))
              * torch.clamp(norm - (lr * lam), min=0.0))
    network.gru.flatten_parameters()

def regularize(network, lam):
    '''Calculate regularization term for first layer weight matrix.'''
    W = network.gru.weight_ih_l0
    return lam * torch.sum(torch.norm(W, dim=0))

def ridge_regularize(network, lam):
    '''Apply ridge penalty at linear layer and hidden-hidden weights.'''
    return lam * (
        torch.sum(network.linear.weight ** 2) +
        torch.sum(network.gru.weight_hh_l0 ** 2))

def restore_parameters(model, best_model):
    '''Move parameter values from best_model to model.'''
    for params, best_params in zip(model.parameters(), best_model.parameters()):
        params.data = best_params

def arrange_input(data, context):
    '''Arrange a single time series into overlapping short sequences.'''
    assert context >= 1 and isinstance(context, int)
    input = torch.zeros(len(data) - context, context, data.shape[1],
                        dtype=torch.float32, device=data.device)
    target = torch.zeros(len(data) - context, context, data.shape[1],
                         dtype=torch.float32, device=data.device)
    for i in range(context):
        start = i
        end = len(data) - context + i
        input[:, i, :] = data[start:end]
        target[:, i, :] = data[start+1:end+1]
    return input.detach(), target.detach()

# Training functions
def train_phase1(crvae, X, context, lr, max_iter, lam=0, lam_ridge=0,
                     lookback=5, check_every=50, verbose=1, sparsity=100,
                     batch_size=2048, lambda_cs=0.01):
    '''Train CR-CSRAE model (Phase 1: Structure Learning).'''
    p = X.shape[-1]
    device = crvae.networks[0].gru.weight_ih_l0.device
    loss_fn = nn.MSELoss()
    train_loss_list = []
    batch_size = batch_size

    # Set up data.
    X, Y = zip(*[arrange_input(x, context) for x in X])
    X_all = torch.cat(X, dim=0)
    Y_all = torch.cat(Y, dim=0)

    # éšæœºæŠ½å–ä¸€ä¸ªmini-batch
    idx = np.random.randint(len(X_all), size=(batch_size,))
    
    X_batch = X_all[idx]
    
    Y_batch = Y_all[idx]

    start_point = 0
    beta = 0.1

    # For early stopping.
    best_it = None
    best_loss = np.inf
    best_model = None
    
    pred, mu, log_var = crvae(X_batch)
    # 1. è®¡ç®—é‡æž„æŸå¤±
    reconstruction_loss = sum([loss_fn(pred[i][:, :, 0], X_batch[:, 10:, i]) for i in range(p)])

    # 2. è®¡ç®—CSæ•£åº¦æ­£åˆ™é¡¹
    mu_q = mu.squeeze(0)
    var_q = log_var.exp().squeeze(0)
    mu_p, var_p = crvae.prior()
    cs_div = cs_divergence_gmm(mu_q, var_q, mu_p, var_p).mean()

    # 3. è®¡ç®—å…¶ä»–æ­£åˆ™é¡¹
    ridge = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
        
    # 4. ç»„åˆæˆå¯å¾®åˆ†çš„æ€»æŸå¤±
    smooth_loss = reconstruction_loss + ridge + lambda_cs * cs_div
    # ä¸»è®­ç»ƒå¾ªçŽ¯
    for it in range(max_iter):

        # 5. åå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°
        smooth_loss.backward()
        
        with torch.no_grad():
            for param in crvae.parameters():
                if param.grad is not None:
                    param.data -= lr * param.grad

        # 6. L1ç¨€ç–æ€§æƒ©ç½š
        if lam > 0:
            for net in crvae.networks:
                prox_update(net, lam, lr)

        crvae.zero_grad()

        pred, mu, log_var = crvae(X_batch)
        # 1. è®¡ç®—é‡æž„æŸå¤±
        reconstruction_loss = sum([loss_fn(pred[i][:, :, 0], X_batch[:, 10:, i]) for i in range(p)])

        # 2. è®¡ç®—CSæ•£åº¦æ­£åˆ™é¡¹
        mu_q = mu.squeeze(0)
        var_q = log_var.exp().squeeze(0)
        mu_p, var_p = crvae.prior()
        cs_div = cs_divergence_gmm(mu_q, var_q, mu_p, var_p).mean()

        # 3. è®¡ç®—å…¶ä»–æ­£åˆ™é¡¹
        ridge = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
        
        # 4. ç»„åˆæˆå¯å¾®åˆ†çš„æ€»æŸå¤±
        smooth_loss = reconstruction_loss + ridge + lambda_cs * cs_div
        
        # æ£€æŸ¥ç‚¹å’Œæ—¥å¿—æ‰“å°
        if (it) % check_every == 0:     
            X_t = X_batch
            Y_t = Y_batch
            
            pred_t, mu_t, log_var_t = crvae(X_t)
            # 1. è®¡ç®—é‡æž„æŸå¤±
            reconstruction_loss_t = sum([loss_fn(pred_t[i][:, :, 0], X_t[:, 10:, i]) for i in range(p)])

            # 2. è®¡ç®—CSæ•£åº¦æ­£åˆ™é¡¹
            mu_q_t = mu_t.squeeze(0)
            var_q_t = log_var_t.exp().squeeze(0)
            mu_p_t, var_p_t = crvae.prior()
            cs_div_t = cs_divergence_gmm(mu_q_t, var_q_t, mu_p_t, var_p_t).mean()

            # 3. è®¡ç®—å…¶ä»–æ­£åˆ™é¡¹
            ridge_t = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
        
            # 4. ç»„åˆæˆå¯å¾®åˆ†çš„æ€»æŸå¤±
            smooth_loss_t = reconstruction_loss_t + ridge_t + lambda_cs * cs_div_t

            nonsmooth = sum([regularize(net, lam) for net in crvae.networks])
            mean_loss = (smooth_loss_t) / p
            
            if verbose > 0:
                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it))
                print('Mean Loss = %f' % mean_loss.item())
                print('Recon Loss = %f' % (reconstruction_loss_t.item() / p))
                print('CS_Div = %f' % cs_div_t.item())
                
                if lam > 0:
                  print('Variable usage = %.2f%%'
                        % (100 * torch.mean(crvae.GC().float())))

            if mean_loss < best_loss:
                best_loss = mean_loss
                best_it = it
                best_model = deepcopy(crvae)
                print(f"*** New best model at iter {best_it} with loss {best_loss:.4f} ***")


            start_point = 0
            predicted_data = crvae(X_t,mode = 'test')
            syn = predicted_data[:,:-1,:].cpu().detach().numpy()
            ori= X_t[:,start_point:,:].cpu().detach().numpy()
            
            syn = MinMaxScaler(syn)
            ori = MinMaxScaler(ori)

        # è®°å½•è®­ç»ƒæŸå¤±
        if it % 10 == 0:
            train_loss_list.append(smooth_loss.item())

        # Restore best model.
    restore_parameters(crvae, best_model)
    
        
        

    return train_loss_list

def train_phase2(crvae, vrae, X, context, lr, max_iter,lam=0, lam_ridge=0,
                     lookback=5, check_every=50, verbose=1, sparsity = 100,
                     batch_size=1024, lambda_cs=0.1, lambda_e=0.1):
    '''Train model with Adam.'''

    optimizer = optim.Adam(vrae.parameters(), lr=1e-3)

    p = X.shape[-1]
    device = crvae.networks[0].gru.weight_ih_l0.device
    loss_fn = nn.MSELoss()
    train_loss_list = []
    batch_size = batch_size

    # å‡†å¤‡æ•°æ®
    X_list, Y_list = zip(*[arrange_input(x, context) for x in X])
    X_all = torch.cat(X_list, dim=0)
    Y_all = torch.cat(Y_list, dim=0)

    idx = np.random.randint(len(X_all), size=(batch_size,))
    
    X_batch = X_all[idx]
    
    Y_batch = Y_all[idx]
    X_v = X_all[batch_size:]
    start_point = 0 #context-10-1
    lambda_cs = 0.1 
    lambda_e = 0.1

    best_it = None
    best_loss = np.inf
    best_crvae_model = None
    best_vrae_model = None

    #è®­ç»ƒä¸»æ¨¡åž‹
    pred, mu, log_var = crvae(X_batch)
    # 1. è®¡ç®—é‡æž„æŸå¤±
    reconstruction_loss = sum([loss_fn(pred[i][:, :, 0], X_batch[:, 10:, i]) for i in range(p)])

    # 2. è®¡ç®—CSæ•£åº¦æ­£åˆ™é¡¹
    mu_q = mu.squeeze(0)
    var_q = log_var.exp().squeeze(0)
    mu_p, var_p = crvae.prior()
    cs_div = cs_divergence_gmm(mu_q, var_q, mu_p, var_p).mean()

    # 3. è®¡ç®—å…¶ä»–æ­£åˆ™é¡¹
    ridge = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
        
    # 4. ç»„åˆæˆå¯å¾®åˆ†çš„æ€»æŸå¤±
    smooth_loss = reconstruction_loss + ridge + lambda_cs * cs_div

    # è®­ç»ƒè¯¯å·®è¡¥å¿ç½‘ç»œ VRAE
    # 1. è®¡ç®—é‡æž„è¯¯å·®
    error = (-torch.stack(pred)[:, :, :, 0].permute(1, 2, 0) + X_batch[:, 10:, :]).detach()

    # 2. VRAE å‰å‘ä¼ æ’­
    pred_e, mu_e, log_var_e = vrae(error)
        
    # 3. VRAE çš„é‡æž„æŸå¤±
    loss_e = loss_fn(pred_e, error)
        
    # 4. ä¿®æ­£ï¼šè¯¯å·®è¡¥å¿æ¨¡å—æ”¯æŒ GMM å…ˆéªŒçš„ CS æ•£åº¦
    mu_e_q = mu_e.squeeze(0)
    var_e_q = log_var_e.exp().squeeze(0)

    if vrae.use_gmm:
        mu_e_p, var_e_p = vrae.prior()
        cs_div_e = cs_divergence_gmm(mu_e_q, var_e_q, mu_e_p, var_e_p).mean()
    else:
        mu_e_p = torch.zeros_like(mu_e_q)
        var_e_p = torch.ones_like(var_e_q)
        cs_div_e = cs_divergence_gmm(mu_e_q, var_e_q, mu_e_p, var_e_p).mean()
        
        # 5. VRAE çš„æ€»æŸå¤±
    smooth_e = loss_e + lambda_e * cs_div_e

    # ä¸»è®­ç»ƒå¾ªçŽ¯
    for it in range(max_iter):
        
        smooth_e.backward()
        if lam == 0:
            optimizer.step()
            optimizer.zero_grad()

        smooth_loss.backward()
        with torch.no_grad():
            for param in crvae.parameters():
                if param.grad is not None:
                    param.data -= lr * param.grad

        if lam > 0:
            for net in crvae.networks:
                prox_update(net, lam, lr)

        crvae.zero_grad()
        
        idx = np.random.randint(len(X_all), size=(batch_size,))

        X_batch = X_all[idx]
    
        Y_batch = Y_all[idx]

        pred, mu, log_var = crvae(X_batch)
        # 1. è®¡ç®—é‡æž„æŸå¤±
        reconstruction_loss = sum([loss_fn(pred[i][:, :, 0], X_batch[:, 10:, i]) for i in range(p)])

        # 2. è®¡ç®—CSæ•£åº¦æ­£åˆ™é¡¹
        mu_q = mu.squeeze(0)
        var_q = log_var.exp().squeeze(0)
        mu_p, var_p = crvae.prior()
        cs_div = cs_divergence_gmm(mu_q, var_q, mu_p, var_p).mean()

        # 3. è®¡ç®—å…¶ä»–æ­£åˆ™é¡¹
        ridge = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
        
        # 4. ç»„åˆæˆå¯å¾®åˆ†çš„æ€»æŸå¤±
        smooth_loss = reconstruction_loss + ridge + lambda_cs * cs_div

        # 1. è®¡ç®—é‡æž„è¯¯å·®
        error = (-torch.stack(pred)[:, :, :, 0].permute(1, 2, 0) + X_batch[:, 10:, :]).detach()

        # 2. VRAE å‰å‘ä¼ æ’­
        pred_e, mu_e, log_var_e = vrae(error)
        
        # 3. VRAE çš„é‡æž„æŸå¤±
        loss_e = loss_fn(pred_e, error)
        
        # 4. ä¿®æ­£ï¼šè¯¯å·®è¡¥å¿æ¨¡å—æ”¯æŒ GMM å…ˆéªŒçš„ CS æ•£åº¦
        mu_e_q = mu_e.squeeze(0)
        var_e_q = log_var_e.exp().squeeze(0)

        if vrae.use_gmm:
            mu_e_p, var_e_p = vrae.prior()
            cs_div_e = cs_divergence_gmm(mu_e_q, var_e_q, mu_e_p, var_e_p).mean()
        else:
            mu_e_p = torch.zeros_like(mu_e_q)
            var_e_p = torch.ones_like(var_e_q)
            cs_div_e = cs_divergence_gmm(mu_e_q, var_e_q, mu_e_p, var_e_p).mean()
        
        # 5. VRAE çš„æ€»æŸå¤±
        smooth_e = loss_e + lambda_e * cs_div_e

        # æ£€æŸ¥ç‚¹å’Œæ—¥å¿—æ‰“å°
        if (it) % check_every == 0:
            X_t = X_batch
            
            with torch.no_grad():
                # è¯„ä¼° CR-CSRAE
                pred_t, mu_t, log_var_t = crvae(X_t)
                loss_t = sum([loss_fn(pred_t[i][:, :, 0], X_t[:, 10:, i]) for i in range(p)])
                ridge_t = sum([ridge_regularize(net, lam_ridge) for net in crvae.networks])
                mu_q_t, var_q_t = mu_t.squeeze(0), log_var_t.exp().squeeze(0)
                mu_p_t, var_p_t = crvae.prior()
                cs_div_t = cs_divergence_gmm(mu_q_t, var_q_t, mu_p_t, var_p_t).mean()
                smooth_t= loss_t + ridge_t + lambda_cs * cs_div_t

                nonsmooth = sum([regularize(net, lam) for net in crvae.networks])
                mean_loss = (smooth_t) / p
              
                
            if verbose > 0:
                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it))
                print('[CR-CSRAE] Mean Loss = %.4f | Recon = %.4f | CS_Div = %.4f'
                      % (mean_loss.item(), (reconstruction_loss.item() / p), cs_div.item()))
                
                print('Loss_e = %.4f | CS_Div_e = %.4f'%(smooth_e.item(),cs_div_e.item()))

                if lam>0:
                  print('Variable usage = %.2f%%'
                        % (100 * torch.mean(crvae.GC().float())))
                  
            if mean_loss < best_loss:
                best_loss = mean_loss
                best_it = it
                best_crvae_model = deepcopy(crvae)
                best_vrae_model = deepcopy(vrae)
                print(f"*** New best models at iter {best_it} with main loss {best_loss:.4f} ***")
            
            start_point = 0
            predicted_error = vrae(error, mode = 'test').detach()
            
            predicted_data = crvae(X_t, predicted_error, mode = 'test', phase = 1)
            syn = predicted_data[:,:-1,:].cpu().detach().numpy()
            ori= X_t[:,start_point:,:].cpu().detach().numpy()

            # å¯è§†åŒ–ç”Ÿæˆæ ·æœ¬
            if it % 1000 == 0:
              plt.plot(ori[0,:,1])
              plt.plot(syn[0,:,1])
              plt.show()

              visualization(ori, syn, 'pca')
              visualization(ori, syn, 'tsne')
              np.save('ori_henon.npy',ori)
              np.save('syn_henon.npy',syn)
        
        # è®°å½•è®­ç»ƒæŸå¤±
        if it % 10 == 0:
            train_loss_list.append(smooth_loss.item())


    restore_parameters(crvae, best_crvae_model)
    restore_parameters(vrae, best_vrae_model)
    print(f"Finished training. Restored best models from iteration {best_it}.")


    return train_loss_list

# Lorenz-96 æ•°æ®ç”Ÿæˆ
def lorenz(x, t, F):
    '''Partial derivatives for Lorenz-96 ODE.'''
    p = len(x)
    dxdt = np.zeros(p)
    for i in range(p):
        dxdt[i] = (x[(i + 1) % p] - x[(i - 2) % p]) * x[(i - 1) % p] - x[i] + F
    return dxdt

def lorenz_96(d, t, t_eval, f, seed, delta_t=0.1, sd=0.1, burn_in=1000):
    if seed is not None:
        np.random.seed(seed)

    # 1. åˆå§‹çŠ¶æ€ä¸Žæ—¶é—´ç‚¹
    x0 = np.random.normal(scale=0.01, size=d)
    tm = np.linspace(0, (t + t_eval + burn_in) * delta_t, t + t_eval + burn_in)

    # 2. æ±‚è§£ ODE å¹¶åŠ å™ªå£°
    X = odeint(lorenz, x0, tm, args=(f,))
    X += np.random.normal(scale=sd, size=(t + t_eval + burn_in, d))

    X_stable = X[burn_in:]

    # 4. æ²¿ time è½´å¯¹æ¯ä¸ªå˜é‡åšæ ‡å‡†åŒ–
    mean = X_stable.mean(axis=0, keepdims=True)
    std  = X_stable.std(axis=0, keepdims=True)
    X_stable = (X_stable - mean) / (std + 1e-8)

    # 5. ç›´æŽ¥è¿”å›ž (time, dim)
    return X_stable.T.astype(np.float32)

# ä¸»ç¨‹åºæ¼”ç¤º
if __name__ == "__main__":
    # å°è¯•åŠ è½½æ•°æ®ï¼Œå¦åˆ™ç”Ÿæˆå¹¶ä¿å­˜
    fname = '2_x.npy'
    try:
        X_np = np.load(fname)
        print(f"Loaded `{fname}` with shape {X_np.shape}")
    except FileNotFoundError:
        X_np = lorenz_96(
            d=10,        # å˜é‡æ•°é‡
            t=2048,      # æ—¶é—´æ­¥æ•°
            t_eval=0,    # ä¸é¢å¤–ç”Ÿæˆæµ‹è¯•æ®µ
            f=10.0,      # Lorenz-96 æ–¹ç¨‹çš„å¸¸æ•° F
            seed=0       # éšæœºç§å­
        )
        np.save(fname, X_np)
        print(f"Generated and saved `{fname}` with shape {X_np.shape}")

    if X_np.ndim == 2:
        X_np = X_np[np.newaxis, :, :]
        print(f"Expanded to 3D with shape {X_np.shape}")

    # è½¬ä¸ºæ¨¡åž‹è¾“å…¥ï¼š(batch, T, dim)
    X = torch.tensor(X_np.transpose(0, 2, 1), dtype=torch.float32, device=device)
    print("Data tensor shape:", X.shape)

    # æž„é€ çœŸå®žå› æžœçŸ©é˜µ
    p = 10
    GC_true = np.zeros((p, p), dtype=int)
    for i in range(p):
        GC_true[i, i]    = 1
        GC_true[i, (i-1)%p] = 1
        GC_true[i, (i-2)%p] = 1
        GC_true[i, (i+1)%p] = 1
    print("True GC matrix:\n", GC_true)

    # åˆ›å»ºæ¨¡åž‹
    full_connect = np.ones(GC_true.shape)
    cgru = CRVAE(X.shape[-1], full_connect, hidden=64, K=10, lambda_cs=0.1).to(device)
    
    vrae = VRAE4E(X.shape[-1], hidden=64, K=10, lambda_cs=0.1).to(device)

    print("Models created successfully")
    print(f"Data shape: {X.shape}")
    print(f"Device: {device}")

    # Phase 1 è®­ç»ƒ
    print("Starting Phase 1 Training...")
    train_loss_list = train_phase1(
        cgru, X, context=20, lam=0.5, lam_ridge=0, lr=5e-2, max_iter=2000,
        check_every=50,batch_size=2048,lambda_cs=0.1)

    # æå–å’Œè¯„ä¼° Granger å› æžœå…³ç³»
    GC_est = cgru.GC().cpu().data.numpy()

    print('True variable usage = %.2f%%' % (100 * np.mean(GC_true)))
    print('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))
    print('Accuracy = %.2f%%' % (100 * np.mean(GC_true == GC_est)))

    # ç»˜åˆ¶å› æžœçŸ©é˜µå¯¹æ¯”å›¾
    fig, axarr = plt.subplots(1, 2, figsize=(10, 5))
    axarr[0].imshow(GC_true, cmap='Blues')
    axarr[0].set_title('True Causal Matrix')
    axarr[0].set_ylabel('Effect series')
    axarr[0].set_xlabel('Causal series')
    axarr[0].set_xticks([])
    axarr[0].set_yticks([])

    axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))
    axarr[1].set_title('Estimated Causal Matrix')
    axarr[1].set_ylabel('Effect series')
    axarr[1].set_xlabel('Causal series')
    axarr[1].set_xticks([])
    axarr[1].set_yticks([])

    # æ ‡è®°ä¸ä¸€è‡´çš„åœ°æ–¹
    for i in range(len(GC_est)):
        for j in range(len(GC_est)):
            if GC_true[i, j] != GC_est[i, j]:
                rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)
                axarr[1].add_patch(rect)

    plt.tight_layout()
    plt.show()

    # ä¿å­˜å­¦ä¹ åˆ°çš„ç»“æž„ç”¨äºŽ Phase 2
    np.save('GC_learned.npy', GC_est)
    full_connect = GC_est  # ä½¿ç”¨å­¦ä¹ åˆ°çš„ç»“æž„

    # Phase 2 è®­ç»ƒ
    print("Starting Phase 2 Training...")
    # ä½¿ç”¨å­¦ä¹ åˆ°çš„ç»“æž„é‡æ–°åˆ›å»ºæ¨¡åž‹
    cgru = CRVAE(X.shape[-1], full_connect, hidden=64, K=10, lambda_cs=0.1).to(device)
    vrae = VRAE4E(X.shape[-1], hidden=64, K=10, lambda_cs=0.1).to(device)

    train_loss_list = train_phase2(
        cgru, vrae, X, context=20, lam=0., lam_ridge=0, lr=5e-2, max_iter=20000,
        check_every=50,batch_size=1024,lambda_cs=0.1, lambda_e=0.1)

    print("Training completed!")
