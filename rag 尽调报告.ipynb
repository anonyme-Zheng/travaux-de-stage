{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736dcc82-8acc-4d0e-ae8e-20a1367b43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch==7.17.12 in d:\\anaconda\\envs\\es\\lib\\site-packages (7.17.12)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from elasticsearch==7.17.12) (1.26.20)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\es\\lib\\site-packages (from elasticsearch==7.17.12) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# å»ºè®®ä¸Ž 7.17.12 æœåŠ¡å™¨ç‰ˆæœ¬ç›¸åŒ¹é…\n",
    "%pip install elasticsearch==7.17.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7972bc4-e4dc-4622-b64f-f1263edc9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588aeb1e-c833-4a46-a304-b2b8a7504316",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],  \n",
    "    request_timeout=30,\n",
    "    max_retries=10,\n",
    "    retry_on_timeout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839b9e6b-af11-49d6-89f8-12eab9e94703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '254304d0edc0', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'O5_RtdGsRaeimBqE05E4HQ', 'version': {'number': '7.17.12', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'e3b0c3d3c5c130e1dc6d567d6baef1c73eeb2059', 'build_date': '2023-07-20T05:33:33.690180787Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "print(es.info())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7425b0-e391-4762-817a-72a84379704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 3,\n",
    "            \"number_of_replicas\": 1\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"chunk\": { \"type\": \"text\" },\n",
    "            \"doc_id\": { \"type\": \"keyword\" },\n",
    "            \"chunk_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1024\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if not es.indices.exists(index=\"dd_report_data\"):  \n",
    "    es.indices.create(index=\"dd_report_data\", body=create_index_body)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed6bed6-f222-4d2f-89cf-4b80e3bd5845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: FlagEmbedding in d:\\anaconda\\envs\\es\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (2.8.0+cu128)\n",
      "Requirement already satisfied: transformers>=4.44.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (4.50.0)\n",
      "Requirement already satisfied: datasets>=2.19.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (3.2.0)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (1.10.1)\n",
      "Requirement already satisfied: sentence_transformers in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (5.1.1)\n",
      "Requirement already satisfied: peft in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (0.15.1)\n",
      "Requirement already satisfied: ir-datasets in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (0.5.11)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (0.2.1)\n",
      "Requirement already satisfied: protobuf in d:\\anaconda\\envs\\es\\lib\\site-packages (from FlagEmbedding) (6.32.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (23.2)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\es\\lib\\site-packages (from datasets>=2.19.0->FlagEmbedding) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.21.0)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.19.0->FlagEmbedding) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp->datasets>=2.19.0->FlagEmbedding) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\es\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2025.10.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from torch>=1.6.0->FlagEmbedding) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\es\\lib\\site-packages (from torch>=1.6.0->FlagEmbedding) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from sympy>=1.13.3->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\es\\lib\\site-packages (from tqdm>=4.66.3->datasets>=2.19.0->FlagEmbedding) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\envs\\es\\lib\\site-packages (from transformers>=4.44.2->FlagEmbedding) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\anaconda\\envs\\es\\lib\\site-packages (from transformers>=4.44.2->FlagEmbedding) (0.21.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (4.13.5)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (2.6.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (6.0.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (4.4.4)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (0.1.10)\n",
      "Requirement already satisfied: ijson>=3.1.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (3.4.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from ir-datasets->FlagEmbedding) (0.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.5)\n",
      "Requirement already satisfied: cbor>=1.0.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\es\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\es\\lib\\site-packages (from sentence_transformers->FlagEmbedding) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\es\\lib\\site-packages (from sentence_transformers->FlagEmbedding) (1.16.2)\n",
      "Requirement already satisfied: Pillow in d:\\anaconda\\envs\\es\\lib\\site-packages (from sentence_transformers->FlagEmbedding) (11.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56490fed-cc3a-4821-ad5d-d14868ea5695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b807355978847cfbbee14f8146c754b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23cae14b-5c1c-48af-82ae-d9a9146f38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    out = model.encode(texts,\n",
    "                    max_length=8192,\n",
    "                    return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
    "    return out[\"dense_vecs\"][0].tolist()  # 1024ç»´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f983289a-e5f2-4787-8d13-4767f506cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.1.16 in d:\\anaconda\\envs\\es\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (0.1.53)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain==0.1.16) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.21.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\envs\\es\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (3.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\es\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (4.10.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\es\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\es\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\envs\\es\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\anaconda\\envs\\es\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.16) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.16) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda\\envs\\es\\lib\\site-packages (from requests<3,>=2->langchain==0.1.16) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from requests<3,>=2->langchain==0.1.16) (1.26.20)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\anaconda\\envs\\es\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in d:\\anaconda\\envs\\es\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\anaconda\\envs\\es\\lib\\site-packages (from jsonlines) (24.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (D:\\anaconda\\envs\\es\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.1.16\n",
    "%pip install jsonlines   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ff6078-1fa1-4e5c-ba2e-701c71e034c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb47a1eb-c8e1-44c2-8fe3-89f6fbba4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8192,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"@@SPLIT@@\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a408e5fa-8149-4c4b-808d-e7ea10194ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    \n",
    "    emb = model.encode(\n",
    "        texts,\n",
    "        batch_size=64,\n",
    "        max_length=4096,      \n",
    "    )\n",
    "  \n",
    "    return emb.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3eb478-26cb-48e5-9832-0597048074f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1168453-c1bc-4db5-8970-09c82428db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import hashlib, uuid, json\n",
    "import jsonlines, hashlib, datetime, re\n",
    "from elasticsearch import helpers, exceptions\n",
    "import random, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6650a297-c087-48f9-b322-4673794ef862",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_DIR    =  Path(r\"C:\\Users\\sxmxs\\Desktop\\dd_rapprot\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf154a34-175b-4b9d-a07c-035fdf632660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶:   0%|          | 0/1 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "for txt_file in tqdm(list(TXT_DIR.rglob(\"*.txt\")), desc=\"æ–‡ä»¶\"):\n",
    "    helpers.bulk(es, gen_actions(txt_file), chunk_size=8192, request_timeout=120)\n",
    "    print(f\"âœ… å·²å†™å…¥ {txt_file.relative_to(TXT_DIR.parent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a073ce-3408-455d-8df8-f8f77f738c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf4fdb7-91fd-421c-bb8b-8a95c55bae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "HARD_TOKEN = \"@@SPLIT@@\"\n",
    "TOKENIZER_ID = \"BAAI/bge-large-zh-v1.5\"  # æ¢æˆä½ çš„åµŒå…¥æ¨¡åž‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID)\n",
    "\n",
    "def split_by_tokens_keep_text(text: str, max_tokens=8192, overlap_tokens=64) -> List[str]:\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "    ids = enc[\"input_ids\"]\n",
    "    offs = enc[\"offset_mapping\"]\n",
    "    step = max_tokens - overlap_tokens\n",
    "    out = []\n",
    "    for start in range(0, len(ids), step):\n",
    "        end = min(len(ids), start + max_tokens)\n",
    "        if start >= len(ids):\n",
    "            break\n",
    "        s_char = offs[start][0]\n",
    "        e_char = offs[end-1][1]\n",
    "        out.append(text[s_char:e_char])  # â† ç›´æŽ¥ä»ŽåŽŸæ–‡æˆªå–ï¼Œæ— ç©ºæ ¼\n",
    "    return out or [text]\n",
    "\n",
    "def split_blocks_then_chunks(text: str, max_tokens=8192, overlap_tokens=64) -> List[Tuple[int,int,str,str,str]]:\n",
    "    \"\"\"\n",
    "    è¿”å›ž [(block_id, part_id, section, chunk_for_embed, chunk_for_display)]\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
    "    blocks = [b.strip() for b in re.split(rf\"\\s*{re.escape(HARD_TOKEN)}\\s*\", text) if b.strip()]\n",
    "    out = []\n",
    "    for bi, blk in enumerate(blocks):\n",
    "        m = re.match(r\"\\s*[-â€¢\\s]*\\*\\*(.+?)\\*\\*\\s*[:ï¼š]\\s*\", blk)\n",
    "        section = (m.group(1).strip() if m else \"\")\n",
    "        parts = split_by_tokens_keep_text(blk, max_tokens, overlap_tokens)\n",
    "        for pj, piece in enumerate(parts):\n",
    "            # ç”¨äºŽå‘é‡çš„æ–‡æœ¬é‡Œæ³¨å…¥ section æç¤ºï¼›å±•ç¤ºç”¨ä¿æŒåŽŸæ–‡\n",
    "            piece_for_embed = f\"[{section}] {piece}\" if section else piece\n",
    "            out.append((bi, pj, section, piece_for_embed, piece))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be50fc2c-aec9-4a13-a038-765f3f653f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_actions(txt_path: Path):\n",
    "    text = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    pieces = split_blocks_then_chunks(text, max_tokens=8192, overlap_tokens=64)\n",
    "    embed_texts = [p[3] for p in pieces]     # å¸¦ [å…¬å¸ç®€ä»‹] çš„æ–‡æœ¬\n",
    "    disp_texts  = [p[4] for p in pieces]     # åŽŸæ–‡å±•ç¤ºæ–‡æœ¬\n",
    "    vecs = embed(embed_texts)                # ç”Ÿæˆ 1024 ç»´å‘é‡ï¼ˆä¸Žä½  mapping ä¸€è‡´ï¼‰\n",
    "\n",
    "    doc_uid = txt_path.stem\n",
    "    for (bi, pj, section, _, disp), vec in zip(pieces, vecs):\n",
    "        yield {\n",
    "            \"_index\": \"dd_report_data\",\n",
    "            \"_id\": f\"{doc_uid}#{bi}#{pj}\",\n",
    "            \"_source\": {\n",
    "                \"doc_id\": doc_uid,\n",
    "                \"block_id\": bi,\n",
    "                \"part_id\": pj,\n",
    "                \"section\": section,\n",
    "                \"section_text\": section,\n",
    "                \"chunk\": disp,              # â† å±•ç¤ºç”¨åŽŸæ–‡ï¼Œæ— ç©ºæ ¼\n",
    "                \"chunk_vector\": vec\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b137bcf2-fed5-4ebb-8767-db4ce3882a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index=\"dd_report_data\", ignore_unavailable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f01cc95-6c83-449c-a597-ce3821204dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_body = {\n",
    "  \"settings\": {\"number_of_shards\": 1, \"number_of_replicas\": 0},\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"doc_id\":       {\"type\": \"keyword\"},\n",
    "      \"block_id\":     {\"type\": \"integer\"},\n",
    "      \"part_id\":      {\"type\": \"integer\"},\n",
    "      \"section\":      {\"type\": \"keyword\"},                # ç²¾ç¡®å€¼ã€è¿‡æ»¤/åŠ æƒ\n",
    "      \"section_text\": {\"type\": \"text\"},                   # BM25 åŒ¹é…â€œå…¬å¸ç®€ä»‹â€\n",
    "      \"chunk\":        {\"type\": \"text\"},\n",
    "      \"chunk_vector\": {\"type\": \"dense_vector\", \"dims\": 1024}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=\"dd_report_data\"):  \n",
    "    es.indices.create(index=\"dd_report_data\", body=create_index_body)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45909f0c-b121-4047-b627-83607560cefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "075d1b11-dec8-47fa-880b-3eb3d4cb1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5090 Laptop GPU\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))  # ä¾‹å¦‚ (9, 0) è¡¨ç¤º sm_90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5103fd17-394c-44a7-91a5-bf3c729d3d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dd_report_data': {'mappings': {'properties': {'block_id': {'type': 'integer'},\n",
      "                                                'chunk': {'type': 'text'},\n",
      "                                                'chunk_vector': {'dims': 1024,\n",
      "                                                                 'type': 'dense_vector'},\n",
      "                                                'doc_id': {'type': 'keyword'},\n",
      "                                                'part_id': {'type': 'integer'},\n",
      "                                                'section': {'type': 'keyword'},\n",
      "                                                'section_text': {'type': 'text'}}}}}\n"
     ]
    }
   ],
   "source": [
    "mapping = es.indices.get_mapping(index=\"dd_report_data\")\n",
    "from pprint import pprint\n",
    "pprint(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "651fe2d3-45a6-4e05-bf34-602f7f6dfb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ ç´¢å¼•åï¼š.geoip_databases\n",
      "ðŸ“Š æ–‡æ¡£æ•°ï¼š0\n",
      "ðŸ’¾ å ç”¨ç£ç›˜ï¼š250b\n",
      "------------------------------\n",
      "ðŸ“¦ ç´¢å¼•åï¼šdd_report_data\n",
      "ðŸ“Š æ–‡æ¡£æ•°ï¼š163\n",
      "ðŸ’¾ å ç”¨ç£ç›˜ï¼š2.2mb\n",
      "------------------------------\n",
      "ðŸ“¦ ç´¢å¼•åï¼šfinancial_report_data\n",
      "ðŸ“Š æ–‡æ¡£æ•°ï¼š71641\n",
      "ðŸ’¾ å ç”¨ç£ç›˜ï¼š1.4gb\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "indices = es.cat.indices(format=\"json\")\n",
    "\n",
    "for idx in indices:\n",
    "    print(f\"ðŸ“¦ ç´¢å¼•åï¼š{idx['index']}\")\n",
    "    print(f\"ðŸ“Š æ–‡æ¡£æ•°ï¼š{idx['docs.count']}\")\n",
    "    print(f\"ðŸ’¾ å ç”¨ç£ç›˜ï¼š{idx['store.size']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af6173f2-02df-4ae9-bc20-c53294aa8980",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m dsl = {\n\u001b[32m      2\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m20\u001b[39m,\n\u001b[32m      3\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscript_score\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      5\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mmatch_all\u001b[39m\u001b[33m\"\u001b[39m: {}},\n\u001b[32m      6\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mscript\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcosineSimilarity(params.qv, doc[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mchunk_vector\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]) + 1.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mqv\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mq_vec\u001b[49m}  \u001b[38;5;66;03m# q_vec æ˜¯ä¸€ä¸ª 1024 ç»´ list\u001b[39;00m\n\u001b[32m      9\u001b[39m       }\n\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m   },\n\u001b[32m     12\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33m_source\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m }\n\u001b[32m     15\u001b[39m resp = es.search(index=\u001b[33m\"\u001b[39m\u001b[33mdd_report_data\u001b[39m\u001b[33m\"\u001b[39m, body=dsl)\n",
      "\u001b[31mNameError\u001b[39m: name 'q_vec' is not defined"
     ]
    }
   ],
   "source": [
    "dsl = {\n",
    "  \"size\": 20,\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\": {\"match_all\": {}},\n",
    "      \"script\": {\n",
    "        \"source\": \"cosineSimilarity(params.qv, doc['chunk_vector']) + 1.0\",\n",
    "        \"params\": {\"qv\": q_vec}  # q_vec æ˜¯ä¸€ä¸ª 1024 ç»´ list\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": True\n",
    "}\n",
    "\n",
    "resp = es.search(index=\"dd_report_data\", body=dsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c5f8e1-114b-4361-a54e-1df2e06576d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m q_text = \u001b[33m\"\u001b[39m\u001b[33mä¸»è¦ç­–ç•¥ä»‹ç»\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m q_vec  = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.tolist()      \n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "q_text = \"ä¸»è¦ç­–ç•¥ä»‹ç»\"\n",
    "q_vec  = model.encode([q_text])[0].tolist()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e567ab1-d1ac-481d-9911-65dcd0fceb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: The vector functions of the form function(query, doc['field']) are deprecated, and the form function(query, 'field') should be used instead. For example, cosineSimilarity(query, doc['field']) is replaced by cosineSimilarity(query, 'field').\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "RequestError(400, 'search_phase_execution_exception', 'runtime error')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRequestError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunction_score\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m: base_bool},\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m         }\n\u001b[32m     43\u001b[39m     }\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ç”¨æ³•ï¼ˆæ–°å®¢æˆ·ç«¯å‚æ•°é£Žæ ¼ï¼‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m resp = \u001b[43mes\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdd_report_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhybrid_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_vec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblock_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpart_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\client\\utils.py:347\u001b[39m, in \u001b[36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    346\u001b[39m         params[p] = kwargs.pop(p)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\client\\__init__.py:1821\u001b[39m, in \u001b[36mElasticsearch.search\u001b[39m\u001b[34m(self, body, index, doc_type, params, headers)\u001b[39m\n\u001b[32m   1818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfrom_\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   1819\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mfrom\u001b[39m\u001b[33m\"\u001b[39m] = params.pop(\u001b[33m\"\u001b[39m\u001b[33mfrom_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_make_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\transport.py:466\u001b[39m, in \u001b[36mTransport.perform_request\u001b[39m\u001b[34m(self, method, url, headers, params, body)\u001b[39m\n\u001b[32m    464\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    469\u001b[39m     \u001b[38;5;66;03m# connection didn't fail, confirm it's live status\u001b[39;00m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.connection_pool.mark_live(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\transport.py:427\u001b[39m, in \u001b[36mTransport.perform_request\u001b[39m\u001b[34m(self, method, url, headers, params, body)\u001b[39m\n\u001b[32m    424\u001b[39m connection = \u001b[38;5;28mself\u001b[39m.get_connection()\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     status, headers_response, data = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[32m    438\u001b[39m     headers_response = {\n\u001b[32m    439\u001b[39m         header.lower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers_response.items()\n\u001b[32m    440\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py:291\u001b[39m, in \u001b[36mUrllib3HttpConnection.perform_request\u001b[39m\u001b[34m(self, method, url, params, body, timeout, ignore, headers)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= response.status < \u001b[32m300\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m response.status \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mself\u001b[39m.log_request_fail(\n\u001b[32m    289\u001b[39m         method, full_url, url, orig_body, duration, response.status, raw_data\n\u001b[32m    290\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.log_request_success(\n\u001b[32m    294\u001b[39m     method, full_url, url, orig_body, response.status, raw_data, duration\n\u001b[32m    295\u001b[39m )\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.status, response_headers, raw_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:328\u001b[39m, in \u001b[36mConnection._raise_error\u001b[39m\u001b[34m(self, status_code, raw_data)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    326\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mUndecodable raw error response from server: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, err)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[32m    329\u001b[39m     status_code, error_message, additional_info\n\u001b[32m    330\u001b[39m )\n",
      "\u001b[31mRequestError\u001b[39m: RequestError(400, 'search_phase_execution_exception', 'runtime error')"
     ]
    }
   ],
   "source": [
    "def hybrid_query(q_text: str, q_vec: list[float], dense_weight: float = 1.0):\n",
    "    # é’ˆå¯¹çŸ­è¯ï¼ˆ<=4 ä¸ªæ±‰å­—ï¼‰æŠŠç¨€ç–æƒé‡è°ƒé«˜ã€å‘é‡æƒé‡è°ƒä½Ž\n",
    "    is_short = len(q_text) <= 4 and all('\\u4e00' <= ch <= '\\u9fff' for ch in q_text)\n",
    "    sparse_boost = 4.0 if is_short else 1.5\n",
    "    dense_w = 0.5 if is_short else dense_weight\n",
    "\n",
    "    base_bool = {\n",
    "        \"should\": [\n",
    "            {   # BM25 è¦†ç›–æ­£æ–‡\n",
    "                \"multi_match\": {\n",
    "                    \"query\": q_text,\n",
    "                    \"fields\": [\"chunk^1\", \"section_text^5\"],\n",
    "                    \"type\": \"most_fields\"\n",
    "                }\n",
    "            },\n",
    "            {   # ç²¾ç¡®å‘½ä¸­ sectionï¼ˆå¦‚â€œå…¬å¸ç®€ä»‹â€ï¼‰å¼ºåŠ›åŠ æƒ\n",
    "                \"term\": {\"section\": {\"value\": q_text, \"boost\": sparse_boost}}\n",
    "            },\n",
    "            {   # è¯ç»„å‘½ä¸­ä¹ŸåŠ æƒ\n",
    "                \"match_phrase\": {\"chunk\": {\"query\": q_text, \"boost\": 3.0}}\n",
    "            }\n",
    "        ],\n",
    "        \"minimum_should_match\": 1\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"function_score\": {\n",
    "            \"query\": {\"bool\": base_bool},\n",
    "            \"boost_mode\": \"sum\",\n",
    "            \"score_mode\": \"sum\",\n",
    "            \"functions\": [\n",
    "                {   # å‘é‡åˆ†\n",
    "                    \"script_score\": {\n",
    "                        \"script\": {\n",
    "                            \"source\": \"cosineSimilarity(params.qv, doc['chunk_vector']) + 1.0\",\n",
    "                            \"params\": {\"qv\": q_vec}\n",
    "                        }\n",
    "                    },\n",
    "                    \"weight\": dense_w\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ç”¨æ³•ï¼ˆæ–°å®¢æˆ·ç«¯å‚æ•°é£Žæ ¼ï¼‰\n",
    "resp = es.search(\n",
    "    index=\"dd_report_data\",\n",
    "    size=20,\n",
    "    query=hybrid_query(q_text, q_vec),\n",
    "    _source=[\"doc_id\",\"block_id\",\"part_id\",\"section\",\"chunk\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e213fde5-9e8b-4dca-b1db-8b8a2dfa62bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresp\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mhits\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhits\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      2\u001b[39m     score = hit[\u001b[33m\"\u001b[39m\u001b[33m_score\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m     source = hit[\u001b[33m\"\u001b[39m\u001b[33m_source\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'resp' is not defined"
     ]
    }
   ],
   "source": [
    "for hit in resp[\"hits\"][\"hits\"]:\n",
    "    score = hit[\"_score\"]\n",
    "    source = hit[\"_source\"]\n",
    "    chunk = source.get(\"chunk\", \"...\")\n",
    "\n",
    "    \n",
    "    print(f\"âœ… [{score:.4f}] \")\n",
    "    print(f\"ðŸ“„ {chunk[:300]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293e992-765f-438a-86da-ac66314e1949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764811d3-084a-4193-87f5-82d00989a925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95111bf4-1014-4f38-aec1-9324e7445087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bec24-e576-495c-8577-b5572cf68716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cdf452f-fcb7-46ea-861e-8951b8860eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²è¿žæŽ¥ http://127.0.0.1:8000/v1 ï¼Œä½¿ç”¨æ¨¡åž‹ï¼šgpt-3.5-turbo\n",
      "â„¹ï¸ è·³è¿‡ Rerankerï¼ˆè®¾ç½® USE_RERANKER=1 å¯å¯ç”¨ï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, sys, traceback\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ---------- ä¾èµ– ----------\n",
    "# pip install \"openai>=1.5.0\"\n",
    "from openai import OpenAI\n",
    "from transformers.utils.versions import require_version\n",
    "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
    "\n",
    "# ---------- OpenAI å…¼å®¹å®¢æˆ·ç«¯ ----------\n",
    "# å¼ºåˆ¶æœ¬åœ°ç›´è¿žï¼Œé¿å…è¢«ç³»ç»Ÿ/å…¬å¸ä»£ç†åŠ«æŒåˆ°å¤–ç½‘å¯¼è‡´ 502\n",
    "os.environ.setdefault(\"NO_PROXY\", \"127.0.0.1,localhost\")\n",
    "\n",
    "API_PORT = os.getenv(\"API_PORT\", \"8000\")\n",
    "BASE_URL = f\"http://127.0.0.1:{API_PORT}/v1\"\n",
    "API_KEY  = os.getenv(\"API_KEY\", \"0\")      # æœ¬åœ°åŽç«¯é€šå¸¸ä¸æ ¡éªŒ\n",
    "PREF_MODEL_ID = os.getenv(\"MODEL_ID\")     # å¯é€‰ï¼šæ‰‹åŠ¨æŒ‡å®šæ¨¡åž‹ idï¼ˆ/v1/models è¿”å›žçš„ idï¼‰\n",
    "\n",
    "# åˆ›å»ºå®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "def pick_model_id(preferred: str | None = None) -> str:\n",
    "    \"\"\"ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ MODEL_IDï¼›å¦åˆ™ä»Ž /v1/models åˆ—è¡¨é‡Œé€‰ä¸€ä¸ªå¯ç”¨ idã€‚\"\"\"\n",
    "    models = client.models.list()\n",
    "    ids = [m.id for m in models.data] if getattr(models, \"data\", None) else []\n",
    "    if not ids:\n",
    "        raise RuntimeError(\"åŽç«¯æœªè¿”å›žä»»ä½•æ¨¡åž‹ï¼šè¯·ç¡®è®¤ API å·²å¯åŠ¨ä¸” /v1/models å¯è®¿é—®ã€‚\")\n",
    "\n",
    "    if preferred:\n",
    "        if preferred in ids:\n",
    "            return preferred\n",
    "        else:\n",
    "            print(f\"âš ï¸ æŒ‡å®šçš„ MODEL_ID='{preferred}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œå°†æ”¹ç”¨ '{ids[0]}'\")\n",
    "    return ids[0]\n",
    "\n",
    "MODEL_ID = pick_model_id(PREF_MODEL_ID)\n",
    "TIMEOUT_S = float(os.getenv(\"API_TIMEOUT\", \"120\"))\n",
    "\n",
    "print(f\"âœ… å·²è¿žæŽ¥ {BASE_URL} ï¼Œä½¿ç”¨æ¨¡åž‹ï¼š{MODEL_ID}\")\n",
    "\n",
    "# ---------- Prompt æ¨¡æ¿ï¼ˆåŽŸæ ·ä¿ç•™ï¼‰ ----------\n",
    "CLASSIFY_TEMPLATE = \"\"\"\n",
    "# role\n",
    "æ„å›¾åˆ†ç±»å™¨\n",
    "## profile\n",
    "æ ¹æ®åŽ†å²å¯¹è¯å¯¹ç”¨æˆ·è¾“å…¥ query è¿›è¡Œæ„å›¾åˆ†ç±»ã€‚\n",
    "\n",
    "[ç±»åˆ«æ ‡ç­¾ä»¥åŠå®šä¹‰]\n",
    "1.é—²èŠ - ä¸æ¶‰åŠä»»åŠ¡ã€ä¿¡æ¯æˆ–å’¨è¯¢çš„éšæ„èŠå¤©å†…å®¹\n",
    "2.æ“¦è¾¹ - æ¶‰åŠæ•æ„Ÿè¯ã€æ€§æš—ç¤ºã€ä¸é€‚å®œå…¬å¼€åœºåˆçš„å†…å®¹\n",
    "3.æ”¿æ²»äººç‰©æ‰¹åˆ¤ - åŒ…å«å¯¹æ”¿åºœã€å®˜å‘˜æˆ–æ”¿ç­–çš„è´Ÿé¢è¯„è®ºï¼ˆä¸åŒ…æ‹¬æ­£å¸¸çš„è´¢ç»æ–°é—»å’¨è¯¢ï¼‰\n",
    "4.é‡‘èžç›¸å…³å’¨è¯¢ - åŒ…æ‹¬ä¸ªè‚¡ã€è¡Œä¸šã€åŸºé‡‘ã€æŒ‡æ ‡ã€è´¢åŠ¡ã€ä¼°å€¼ã€å¸‚åœºåŠ¨æ€ã€ç»æµŽæ•°æ®ç­‰é‡‘èžæŠ•èµ„ç±»é—®é¢˜\n",
    "\n",
    "ã€å‚è€ƒä¾‹å­ã€‘\n",
    "query: pussy\n",
    "label: æ“¦è¾¹\n",
    "\n",
    "query: ç‰¹æœ—æ™®é€€ä¼‘äº†å—\n",
    "label: æ”¿æ²»äººç‰©æ‰¹åˆ¤\n",
    "\n",
    "query: èŒ…å°è‚¡ç¥¨æ³¢åŠ¨å¤§å—\n",
    "label: é‡‘èžç›¸å…³å’¨è¯¢\n",
    "\n",
    "query: è´µå·žèŒ…å°ä»Šå¹´è´¢æŠ¥å‘å¸ƒäº†å—\n",
    "label: é‡‘èžç›¸å…³å’¨è¯¢\n",
    "\n",
    "query: ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²ï¼Ÿ\n",
    "label: é—²èŠ\n",
    "\n",
    "[åŽ†å²å¯¹è¯]ï¼ˆå¦‚æžœæ— åŽ†å²è¯·å¿½ç•¥ï¼‰\n",
    "{history_dialogue}\n",
    "\n",
    "ã€ç”¨æˆ·queryã€‘\n",
    "{query}\n",
    "\n",
    "è¯·ç›´æŽ¥è¾“å‡º query çš„æ„å›¾ç»“æžœã€‚\n",
    "\"\"\".strip()\n",
    "\n",
    "QUERY_REWRITE_TEMPLATE = \"\"\"\n",
    "æ ¹æ®ä¸Šä¸‹æ–‡åŽ†å²å¯¹è¯å¯¹ query æ”¹å†™ï¼Œè¿›è¡Œè¡¥å…¨/æŒ‡ä»£/å®Œæ•´çš„æ”¹å†™ï¼Œéžå£è¯­åŒ–ï¼Œå…³é”®è¯æå–ï¼Œä¾¿äºŽåšæ£€ç´¢\n",
    "\n",
    "ã€åŽ†å²å¯¹è¯ã€‘æœ€è¿‘5è½®\n",
    "{history_dialogue}\n",
    "\n",
    "ã€queryã€‘\n",
    "{query}\n",
    "\n",
    "ã€å‚è€ƒä¾‹å­ã€‘\n",
    "- åŽ†å²å¯¹è¯\n",
    "human: æˆ‘æƒ³é—®ä¸€ä¸‹aæ¬¾è¿™ä¸ªè½¦æ˜¯å‰é©±è¿˜æ˜¯åŽé©±\n",
    "bot: åŽé©±\n",
    "human: é‚£bå‘¢\n",
    "- è¾“å‡ºç»“æžœ\n",
    "æ”¹å†™åŽçš„query: é‚£bæ¬¾çš„è½¦å­æ˜¯å‰é©±è¿˜æ˜¯åŽé©±?\n",
    "æå–çš„å…³é”®è¯:bæ¬¾, å‰é©±, åŽé©±, æ±½è½¦\n",
    "\n",
    "è¯·ç›´æŽ¥è¾“å‡ºæ”¹å†™ç»“æžœã€‚\n",
    "\n",
    "æ”¹å†™åŽçš„query:\n",
    "æå–çš„å…³é”®è¯:\n",
    "\"\"\".lstrip()\n",
    "\n",
    "ANSWER_PROMPT_TMPL = \"\"\"\n",
    "# è§’è‰²ï¼šé‡‘èžå’¨è¯¢åŠ©æ‰‹\n",
    "- åªå›žç­”ä¸Žé‡‘èžæŠ•èµ„/å®è§‚ç»æµŽ/è´¢æŠ¥è§£è¯»ç›¸å…³çš„é—®é¢˜  \n",
    "- å›žç­”å¿…é¡»åŸºäºŽã€èµ„æ–™å—ã€‘ç»™å‡ºçš„äº‹å®žï¼Œè‹¥æ— ä¿¡æ¯è¯·å›žç­” â€œæˆ‘ä¸ç¡®å®šâ€\n",
    "\n",
    "## è§„åˆ™\n",
    "1. å†…å®¹å¥åº·å‹å¥½ï¼Œè¯­è¨€ç®€æ´æ˜Žäº†ï¼Œæœ‰é€»è¾‘\n",
    "2. å…ˆç»™ç»“è®ºï¼Œå†åˆ—å¼•ç”¨ç¼–å·ï¼›ç»“è®ºä¹‹å¤–ä¸è¦é€éœ²æŽ¨ç†è¿‡ç¨‹\n",
    "3. æ— æ³•å›žç­”æ—¶ç›´æŽ¥è¯´ â€œæˆ‘ä¸ç¡®å®šâ€ï¼Œä¸è¦ç¼–é€ \n",
    "4. å›žç­”ä¸­çš„ä»»ä½•æ•°å­—/äº‹å®žéƒ½è¦èƒ½åœ¨å¼•ç”¨é‡Œæ‰¾åˆ°\n",
    "\n",
    "## è¾“å…¥\n",
    "### èµ„æ–™å—ï¼ˆå·²æŒ‰ç›¸å…³åº¦æŽ’åºï¼‰\n",
    "{context_blocks}\n",
    "\n",
    "### ç”¨æˆ·é—®é¢˜\n",
    "{query}\n",
    "\n",
    "## è¾“å‡ºæ ¼å¼\n",
    "ç­”æ¡ˆï¼š<ä¸€æ®µåˆ°ä¸¤æ®µç›´æŽ¥ç»™å‡ºç»“è®º>  \n",
    "å¼•ç”¨ï¼š<ç”¨ â‘ â‘¡â‘¢â€¦ æ ‡æ³¨ç”¨åˆ°çš„èµ„æ–™ç¼–å·ï¼Œå¯å¤šä¸ªï¼Œä¾‹å¦‚â€œâ‘ â‘¢â€>\n",
    "\"\"\".strip()\n",
    "\n",
    "# ---------- OpenAI ç‰ˆ llm_invoke ----------\n",
    "def llm_invoke(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = \"\",\n",
    "    *,\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 512,\n",
    "    top_p: float = 0.7,\n",
    ") -> str:\n",
    "    # OpenAI å‚æ•°å« max_tokens\n",
    "    EPS = 1e-3\n",
    "    t = max(float(temperature), EPS)\n",
    "    p = max(float(top_p), EPS)\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=messages,\n",
    "            temperature=t,\n",
    "            top_p=p,\n",
    "            max_tokens=int(max_new_tokens),\n",
    "            timeout=TIMEOUT_S,\n",
    "        )\n",
    "        return (resp.choices[0].message.content or \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(\"âŒ è°ƒç”¨èŠå¤©æŽ¥å£å¤±è´¥ï¼š\", e)\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "# ---------- å·¥å…·å‡½æ•°ï¼ˆåŽŸæ ·ä¿ç•™/å¾®è°ƒï¼‰ ----------\n",
    "def build_history(dialogue: list, max_turns: int = 5) -> str:\n",
    "    pairs = []\n",
    "    for m in dialogue[-max_turns*2:]:\n",
    "        role = \"human\" if m[\"role\"] == \"user\" else \"bot\"\n",
    "        pairs.append(f\"{role}: {m['content']}\")\n",
    "    return \"\\n\".join(pairs) or \"ï¼ˆæ— åŽ†å²ï¼‰\"\n",
    "\n",
    "def classify_query(history_dialogue: str, query: str) -> str:\n",
    "    prompt = CLASSIFY_TEMPLATE.format(history_dialogue=history_dialogue, query=query)\n",
    "    sys = \"åªè¾“å‡ºä¸€ä¸ªä¸­æ–‡æ ‡ç­¾ï¼šé—²èŠ / æ“¦è¾¹ / æ”¿æ²»äººç‰©æ‰¹åˆ¤ / é‡‘èžç›¸å…³å’¨è¯¢ã€‚ä¸è¦é™„åŠ ä»»ä½•è§£é‡Šæˆ–å‰åŽç¼€ã€‚\"\n",
    "    out = llm_invoke(prompt, system_prompt=sys, temperature=0.01, max_new_tokens=64, top_p=0.1)\n",
    "    out = re.sub(r'^\\s*(label[:ï¼š]?)\\s*', '', out or '').strip()\n",
    "    return out\n",
    "\n",
    "def _parse_rewrite_output(text: str, fallback_query: str):\n",
    "    text = text or \"\"\n",
    "    m_q  = re.search(r\"æ”¹å†™åŽçš„?query\\s*[:ï¼š]\\s*(.+)\", text)\n",
    "    m_kw = re.search(r\"æå–çš„?å…³é”®è¯\\s*[:ï¼š]\\s*(.+)\", text)\n",
    "    rewrite  = (m_q.group(1).strip() if m_q else fallback_query).strip()\n",
    "    keywords = [k.strip() for k in (m_kw.group(1).split(\",\"))] if m_kw else []\n",
    "    rewrite  = rewrite.strip(\"`\").strip()\n",
    "    keywords = [k for k in keywords if k]\n",
    "    return rewrite, keywords\n",
    "\n",
    "def rewrite_query(dialogue: list, query: str):\n",
    "    hist = build_history(dialogue)\n",
    "    prompt = QUERY_REWRITE_TEMPLATE.format(history_dialogue=hist, query=query)\n",
    "    sys = \"ä¸¥æ ¼æŒ‰æ¨¡æ¿é”®åè¾“å‡ºï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚\"\n",
    "    content = llm_invoke(prompt, system_prompt=sys, temperature=0.3, max_new_tokens=256)\n",
    "    rewrite, keywords = _parse_rewrite_output(content, fallback_query=query)\n",
    "    return rewrite, keywords, content\n",
    "\n",
    "def build_context(passages: List[str], top_k: int = 5) -> str:\n",
    "    items = [p for p in passages[:top_k] if p and p.strip()]\n",
    "    circled_nums = \"â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\"\n",
    "    return \"\\n\\n\".join(f\"{circled_nums[i]} {p}\" for i, p in enumerate(items))\n",
    "\n",
    "def generate_answer(passages: List[str], query: str) -> str:\n",
    "    context = build_context(passages)\n",
    "    prompt  = ANSWER_PROMPT_TMPL.format(context_blocks=context, query=query)\n",
    "    sys = \"ä½ æ˜¯ä¸“ä¸šçš„é‡‘èžå’¨è¯¢åŠ©æ‰‹ã€‚\"\n",
    "    return llm_invoke(prompt, system_prompt=sys, temperature=0.2, max_new_tokens=512)\n",
    "\n",
    "# ---------- ï¼ˆå¯é€‰ï¼‰Rerankerï¼ˆä¿æŒä½ çš„é™çº§é€»è¾‘ï¼‰ ----------\n",
    "# try:\n",
    "#    from FlagEmbedding import FlagReranker\n",
    "#    import torch\n",
    "#    _device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#    reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=_device==\"cuda\")\n",
    "# except Exception as e:\n",
    "#    reranker = None\n",
    "#    print(\"âš ï¸ æœªå¯ç”¨ Rerankerï¼ˆFlagEmbedding æœªå®‰è£…æˆ–æ¨¡åž‹ä¸å¯ç”¨ï¼‰ï¼š\", e)\n",
    "\n",
    "def rerank_documents(query: str, initial_docs: List[str], top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    if not initial_docs:\n",
    "        return []\n",
    "    if reranker is None:\n",
    "       return [{\"score\": 0.0, \"content\": c} for c in initial_docs[:top_n]]\n",
    "    sentence_pairs = [[query, passage] for passage in initial_docs]\n",
    "    scores = reranker.compute_score(sentence_pairs)\n",
    "#   scored = [{\"score\": float(s), \"content\": c} for s, c in zip(scores, initial_docs)]\n",
    "    return sorted(scored, key=lambda x: x[\"score\"], reverse=True)[:top_n]\n",
    "\n",
    "# ===== åœ¨ã€ŒRerankerã€è¿™ä¸€æ®µæ›¿æ¢æˆä¸‹é¢è¿™æ®µ =====\n",
    "USE_RERANKER = os.getenv(\"USE_RERANKER\", \"0\") == \"1\"        # é»˜è®¤å…³é—­ï¼Œæƒ³å¼€æ—¶è®¾ç½®çŽ¯å¢ƒå˜é‡ USE_RERANKER=1\n",
    "RERANKER_ID = os.getenv(\"RERANKER_ID\", \"BAAI/bge-reranker-v2-m3\")\n",
    "RERANKER_LOCAL_ONLY = os.getenv(\"RERANKER_LOCAL_ONLY\", \"1\") == \"1\"  # é»˜è®¤ä»…ç¦»çº¿åŠ è½½ï¼Œé¿å…ç½‘ç»œè¶…æ—¶\n",
    "\n",
    "reranker = None\n",
    "if USE_RERANKER:\n",
    "    try:\n",
    "        import torch\n",
    "        from FlagEmbedding import FlagReranker\n",
    "        if RERANKER_LOCAL_ONLY:\n",
    "            os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")  # æ²¡æœ‰æœ¬åœ°ç¼“å­˜ä¼šç«‹åˆ»æŠ¥é”™è€Œä¸æ˜¯åå¤é‡è¯•\n",
    "        reranker = FlagReranker(\n",
    "            RERANKER_ID,\n",
    "            use_fp16=torch.cuda.is_available(),\n",
    "            local_files_only=RERANKER_LOCAL_ONLY,\n",
    "        )\n",
    "        print(f\"âœ… Reranker å·²å¯ç”¨ï¼š{RERANKER_ID}\")\n",
    "    except Exception as e:\n",
    "        reranker = None\n",
    "        print(\"âš ï¸ æœªå¯ç”¨ Rerankerï¼ˆé™çº§è¿è¡Œï¼‰ï¼š\", e)\n",
    "else:\n",
    "    print(\"â„¹ï¸ è·³è¿‡ Rerankerï¼ˆè®¾ç½® USE_RERANKER=1 å¯å¯ç”¨ï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "# ---------- Elasticsearch ç¤ºä¾‹ï¼ˆä¸Žä½ åŽŸæ¥çš„å®žçŽ°ä¿æŒä¸€è‡´ï¼Œå ä½ï¼‰ ----------\n",
    "# ---------- Elasticsearchï¼šæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ + section åŠ æƒï¼‰ ----------\n",
    "def _has_section_fields(index_name: str = \"dd_report_data\") -> bool:\n",
    "    try:\n",
    "        props = es.indices.get_mapping(index=index_name)[index_name][\"mappings\"][\"properties\"]\n",
    "        return (\"section\" in props) or (\"section_text\" in props)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _hybrid_query_dual(\n",
    "    q1_text: str, q1_vec: list[float],\n",
    "    q2_text: str | None, q2_vec: list[float] | None,\n",
    "    has_section: bool,\n",
    "    dense_weight: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    q1 = æ”¹å†™åŽçš„ queryï¼ˆä¸»æŸ¥è¯¢ï¼Œæƒé‡æ›´é«˜ï¼‰\n",
    "    q2 = åŽŸå§‹ç”¨æˆ·è¾“å…¥ï¼ˆå‰¯æŸ¥è¯¢ï¼Œæƒé‡ç¨ä½Žï¼›ä¸ºç©ºåˆ™ä¸ä½¿ç”¨ï¼‰\n",
    "    \"\"\"\n",
    "    # çŸ­æŸ¥è¯¢ï¼ˆ<=4æ±‰å­—ï¼‰åŠ å¤§ç¨€ç–ã€é™ä½Žå‘é‡\n",
    "    def _short(q: str) -> bool:\n",
    "        return len(q) <= 4 and all('\\u4e00' <= ch <= '\\u9fff' for ch in q)\n",
    "\n",
    "    is_short_q1 = _short(q1_text)\n",
    "    dense_w_q1  = 0.5 if is_short_q1 else dense_weight\n",
    "    sparse_boost_q1 = 4.0 if is_short_q1 else 1.5\n",
    "\n",
    "    is_short_q2 = _short(q2_text) if q2_text else False\n",
    "    dense_w_q2  = 0.4 if is_short_q2 else 0.7 * dense_weight\n",
    "    sparse_boost_q2 = 3.0 if is_short_q2 else 1.2\n",
    "\n",
    "    fields = [\"chunk^1\"]\n",
    "    if has_section:\n",
    "        fields.append(\"section_text^5\")\n",
    "\n",
    "    should_clauses = [\n",
    "        # q1ï¼ˆæ”¹å†™ï¼‰â€”â€”ä¸»åŠ›\n",
    "        {\"multi_match\": {\"query\": q1_text, \"fields\": fields, \"type\": \"most_fields\", \"boost\": 2.0}},\n",
    "        {\"match_phrase\": {\"chunk\": {\"query\": q1_text, \"boost\": 3.0}}},\n",
    "    ]\n",
    "    if has_section:\n",
    "        should_clauses.append({\"term\": {\"section\": {\"value\": q1_text, \"boost\": sparse_boost_q1}}})\n",
    "\n",
    "    # q2ï¼ˆåŽŸå§‹ï¼‰â€”â€”è¾…åŠ©\n",
    "    if q2_text and q2_text.strip() and q2_text.strip() != q1_text.strip():\n",
    "        should_clauses.extend([\n",
    "            {\"multi_match\": {\"query\": q2_text, \"fields\": fields, \"type\": \"most_fields\", \"boost\": 1.2}},\n",
    "            {\"match_phrase\": {\"chunk\": {\"query\": q2_text, \"boost\": 2.0}}},\n",
    "        ])\n",
    "        if has_section:\n",
    "            should_clauses.append({\"term\": {\"section\": {\"value\": q2_text, \"boost\": sparse_boost_q2}}})\n",
    "\n",
    "    base_bool = {\"should\": should_clauses, \"minimum_should_match\": 1}\n",
    "\n",
    "    functions = [{\n",
    "        \"script_score\": {\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.qv1, doc['chunk_vector']) + 1.0\",\n",
    "                \"params\": {\"qv1\": q1_vec}\n",
    "            }\n",
    "        },\n",
    "        \"weight\": dense_w_q1\n",
    "    }]\n",
    "    if q2_vec is not None:\n",
    "        functions.append({\n",
    "            \"script_score\": {\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.qv2, doc['chunk_vector']) + 1.0\",\n",
    "                    \"params\": {\"qv2\": q2_vec}\n",
    "                }\n",
    "            },\n",
    "            \"weight\": dense_w_q2\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"function_score\": {\n",
    "            \"query\": {\"bool\": base_bool},\n",
    "            \"boost_mode\": \"sum\",\n",
    "            \"score_mode\": \"sum\",\n",
    "            \"functions\": functions\n",
    "        }\n",
    "    }\n",
    "\n",
    "def es_hybrid_search(\n",
    "    rewrite_text: str,\n",
    "    raw_text: str | None = None,\n",
    "    index_name: str = \"dd_report_data\",\n",
    "    size: int = 20,\n",
    "):\n",
    "    # ç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼ˆç›¸åŒåˆ™åªç®—ä¸€æ¬¡ï¼‰\n",
    "    vec1 = model.encode([rewrite_text])[0].tolist()         # noqa: F821\n",
    "    vec2 = None\n",
    "    if raw_text and raw_text.strip() and raw_text.strip() != rewrite_text.strip():\n",
    "        vec2 = model.encode([raw_text])[0].tolist()         # noqa: F821\n",
    "\n",
    "    has_section = _has_section_fields(index_name)\n",
    "    query_body = _hybrid_query_dual(\n",
    "        q1_text=rewrite_text, q1_vec=vec1,\n",
    "        q2_text=raw_text, q2_vec=vec2,\n",
    "        has_section=has_section,\n",
    "        dense_weight=1.0\n",
    "    )\n",
    "\n",
    "    return es.search(                                      # noqa: F821\n",
    "        index=index_name,\n",
    "        size=size,\n",
    "        query=query_body,\n",
    "        _source=[\"doc_id\",\"block_id\",\"part_id\",\"section\",\"section_text\",\"chunk\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# ---------- äº¤äº’ä¸»å¾ªçŽ¯ï¼ˆæ— è”ç½‘æœç´¢ï¼‰ ----------\n",
    "def main():\n",
    "    print(\"âœ… å°±ç»ªï¼šè¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ exit é€€å‡ºã€‚\")\n",
    "    conversation: List[Dict[str, str]] = []\n",
    "    turn = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\nBye!\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        turn += 1\n",
    "        print(\"\\n\" + \"â”€\" * 80)\n",
    "        print(f\"ðŸ—£ï¸ ç¬¬ {turn} è½® - ç”¨æˆ·é—®é¢˜ï¼š{user_input}\", flush=True)\n",
    "        print(\"â”€\" * 80 + \"\\n\")\n",
    "\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # 1) åˆ†ç±»\n",
    "        history_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in conversation[-6:-1]])\n",
    "        label = classify_query(history_text, user_input)\n",
    "        print(\"ðŸ“Œ åˆ†ç±»ç»“æžœ:\", label)\n",
    "\n",
    "        if \"é‡‘èž\" not in label:\n",
    "            print(\"âŒ éžé‡‘èžé—®é¢˜ï¼Œä¸äºˆå¤„ç†\")\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": \"å¯¹ä¸èµ·ï¼Œæˆ‘åªèƒ½å¤„ç†é‡‘èžç›¸å…³é—®é¢˜ã€‚\"})\n",
    "            continue\n",
    "\n",
    "        # 2) æ”¹å†™\n",
    "        rewrite, keywords, raw = rewrite_query(conversation, user_input)\n",
    "        print(\"âœï¸ æ”¹å†™åŽçš„ Query:\", rewrite)\n",
    "        print(\"ðŸ”‘ æå–çš„å…³é”®è¯:\", keywords)\n",
    "\n",
    "        # 3) ä»Ž ES å¬å›ž\n",
    "        try:\n",
    "            resp = es_hybrid_search(\n",
    "                        rewrite_text=rewrite,\n",
    "                        raw_text=user_input,                  # â† åŒæ—¶ç”¨åŽŸå§‹è¾“å…¥\n",
    "                        index_name=\"dd_report_data\",\n",
    "                        size=20)\n",
    "            kb_ctx = [h[\"_source\"].get(\"chunk\",\"\") for h in resp[\"hits\"][\"hits\"]]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ ES æŸ¥è¯¢å¤±è´¥ï¼ŒæŒ‰æ— æœ¬åœ°çŸ¥è¯†å¤„ç†ï¼š\", e)\n",
    "            kb_ctx = []\n",
    "\n",
    "        if kb_ctx:\n",
    "            print(f\"ðŸ“š æ¥è‡ª ES çš„å€™é€‰æ®µè½ï¼š{len(kb_ctx)} æ¡\")\n",
    "        else:\n",
    "            print(\"ðŸ“š ES æš‚æ— å‘½ä¸­\")\n",
    "\n",
    "        # 4) é‡æŽ’åºï¼ˆå¯é€‰ï¼‰\n",
    "        merged   = kb_ctx\n",
    "        top_docs = rerank_documents(rewrite, merged, top_n=10)\n",
    "        passages_for_answer = [d[\"content\"] for d in top_docs] if top_docs else merged\n",
    "\n",
    "        # 5) ç”Ÿæˆç­”æ¡ˆï¼ˆåªç”¨æœ¬åœ°çŸ¥è¯†ï¼‰\n",
    "        answer = generate_answer(passages_for_answer, query=rewrite)\n",
    "        print(\"\\nðŸ§  æ¨¡åž‹å›žç­”ï¼š\\n\", answer)\n",
    "\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6510376c-e518-44ae-965b-b2fe9c423d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å°±ç»ªï¼šè¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ exit é€€å‡ºã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15de127b-5e20-4c8b-849e-5d19ad0cdbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sxmxs\\AppData\\Local\\Temp\\ipykernel_9012\\1948768807.py:85: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=420, show_copy_button=True, label=\"å¯¹è¯\")\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 7861-7861. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# ä½œä¸ºè„šæœ¬å¯åŠ¨\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# å¦‚æžœéœ€è¦å…¬ç½‘è®¿é—®å¯ä¼  share=Trueï¼›æœåŠ¡å™¨çŽ¯å¢ƒå¯æ”¹ server_name=\"0.0.0.0\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43mdemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7861\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\gradio\\blocks.py:2794\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[32m   2789\u001b[39m     (\n\u001b[32m   2790\u001b[39m         server_name,\n\u001b[32m   2791\u001b[39m         server_port,\n\u001b[32m   2792\u001b[39m         local_url,\n\u001b[32m   2793\u001b[39m         server,\n\u001b[32m-> \u001b[39m\u001b[32m2794\u001b[39m     ) = \u001b[43mhttp_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2801\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2802\u001b[39m \u001b[38;5;28mself\u001b[39m.server_name = server_name\n\u001b[32m   2803\u001b[39m \u001b[38;5;28mself\u001b[39m.local_url = local_url\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\envs\\es\\Lib\\site-packages\\gradio\\http_server.py:157\u001b[39m, in \u001b[36mstart_server\u001b[39m\u001b[34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    158\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    162\u001b[39m     path_to_local_server = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Cannot find empty port in range: 7861-7861. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "# ====================== Gradio UI ======================\n",
    "import gradio as gr\n",
    "import datetime as _dt\n",
    "\n",
    "DEFAULT_INDEX = \"dd_report_data\"\n",
    "\n",
    "def _history_to_dialogue(chat_history: list[tuple[str, str]]) -> list[Dict[str, str]]:\n",
    "    \"\"\"Gradio Chatbot çš„åŽ†å² (user, assistant) -> ä½ çŽ°æœ‰å‡½æ•°ä½¿ç”¨çš„ [{'role', 'content'}...]\"\"\"\n",
    "    conv = []\n",
    "    for u, a in chat_history:\n",
    "        if u:\n",
    "            conv.append({\"role\": \"user\", \"content\": u})\n",
    "        if a:\n",
    "            conv.append({\"role\": \"assistant\", \"content\": a})\n",
    "    return conv\n",
    "\n",
    "def _format_hits_md(resp, topn: int = 8) -> str:\n",
    "    if not resp or \"hits\" not in resp or not resp[\"hits\"][\"hits\"]:\n",
    "        return \"ï¼ˆæ— å‘½ä¸­ï¼‰\"\n",
    "    lines = []\n",
    "    for i, h in enumerate(resp[\"hits\"][\"hits\"][:topn], 1):\n",
    "        sc = h.get(\"_score\", 0.0)\n",
    "        src = h.get(\"_source\", {})\n",
    "        sec = src.get(\"section\") or src.get(\"section_text\") or \"â€”\"\n",
    "        txt = (src.get(\"chunk\") or \"\").strip().replace(\"\\r\\n\", \"\\n\")\n",
    "        snippet = (txt[:400] + \"â€¦\") if len(txt) > 400 else txt\n",
    "        lines.append(f\"**{i:02d}. [{sc:.3f}] {sec}**\\n\\n{snippet}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(lines)\n",
    "\n",
    "def respond(message, chat_history, temperature, n_ctx, index_name):\n",
    "    \"\"\"\n",
    "    Gradio å›žè°ƒï¼šè¾“å…¥ message å’ŒåŽ†å²ï¼Œè¿”å›žï¼ˆæ›´æ–°åŽçš„åŽ†å²ï¼Œå‘½ä¸­æ–‡æ¡£ã€è°ƒè¯•ä¿¡æ¯ï¼‰\n",
    "    \"\"\"\n",
    "    # 1) åŽ†å²è½¬æ¢ + åˆ†ç±»\n",
    "    conversation = _history_to_dialogue(chat_history)\n",
    "    history_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in conversation[-6:]])\n",
    "    label = classify_query(history_text, message)\n",
    "\n",
    "    if \"é‡‘èž\" not in label:\n",
    "        assistant = \"å¯¹ä¸èµ·ï¼Œæˆ‘åªèƒ½å¤„ç†é‡‘èžç›¸å…³é—®é¢˜ã€‚\"\n",
    "        chat_history = chat_history + [(message, assistant)]\n",
    "        debug_md = f\"**åˆ†ç±»**ï¼š{label}  \\n**æ—¶é—´**ï¼š{_dt.datetime.now()}\"\n",
    "        return chat_history, \"ï¼ˆæ— æ£€ç´¢ï¼‰\", debug_md\n",
    "\n",
    "    # 2) æ”¹å†™\n",
    "    rewrite, keywords, _raw = rewrite_query(conversation + [{\"role\": \"user\", \"content\": message}], message)\n",
    "\n",
    "    # 3) æ£€ç´¢ï¼ˆåŒæŸ¥è¯¢ï¼šåŽŸå§‹ + æ”¹å†™ï¼‰\n",
    "    try:\n",
    "        resp = es_hybrid_search(              # â† ä½ ä¸Šé¢å®šä¹‰çš„æ–°å‡½æ•°\n",
    "            rewrite_text=rewrite,\n",
    "            raw_text=message,\n",
    "            index_name=index_name,\n",
    "            size=n_ctx\n",
    "        )\n",
    "        kb_ctx = [h[\"_source\"].get(\"chunk\", \"\") for h in resp[\"hits\"][\"hits\"]]\n",
    "        hits_md = _format_hits_md(resp, topn=min(8, n_ctx))\n",
    "    except Exception as e:\n",
    "        kb_ctx, hits_md = [], f\"æ£€ç´¢å¤±è´¥ï¼š{e!r}\"\n",
    "\n",
    "    # 4) ï¼ˆå¯é€‰ï¼‰é‡æŽ’åº\n",
    "    merged   = kb_ctx\n",
    "    top_docs = rerank_documents(rewrite, merged, top_n=min(10, len(merged)))\n",
    "    passages_for_answer = [d[\"content\"] for d in top_docs] if top_docs else merged\n",
    "\n",
    "    # 5) ç”Ÿæˆ\n",
    "    _old_temp = 0.2\n",
    "    assistant = generate_answer(passages_for_answer[:5], query=rewrite)  # åªå–å‰5æ®µå…¥ä¸Šä¸‹æ–‡\n",
    "    chat_history = chat_history + [(message, assistant)]\n",
    "\n",
    "    # 6) è°ƒè¯•ä¿¡æ¯\n",
    "    debug_md = (\n",
    "        f\"**åˆ†ç±»**ï¼š{label}\\n\\n\"\n",
    "        f\"**åŽŸå§‹é—®é¢˜**ï¼š{message}\\n\\n\"\n",
    "        f\"**æ”¹å†™åŽ**ï¼š{rewrite}\\n\\n\"\n",
    "        f\"**å…³é”®è¯**ï¼š{', '.join(keywords) if keywords else 'â€”'}\\n\\n\"\n",
    "        f\"**æ—¶é—´**ï¼š{_dt.datetime.now()}\"\n",
    "    )\n",
    "    return chat_history, hits_md, debug_md\n",
    "\n",
    "with gr.Blocks(title=\"å°½è°ƒé—®ç­”åŠ©æ‰‹\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## å°½è°ƒé—®ç­”åŠ©æ‰‹\\nè¾“å…¥é—®é¢˜ï¼Œæˆ‘ä¼šåŸºäºŽæœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢å¹¶å›žç­”ã€‚\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=420, show_copy_button=True, label=\"å¯¹è¯\")\n",
    "        with gr.Column():\n",
    "            hits = gr.Markdown(label=\"å‘½ä¸­æ–‡æ¡£ï¼ˆTopï¼‰\", value=\"ï¼ˆç­‰å¾…æ£€ç´¢â€¦ï¼‰\")\n",
    "            dbg  = gr.Markdown(label=\"è°ƒè¯•ä¿¡æ¯\", value=\"â€”\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"ä¾‹å¦‚ï¼šå…¬å¸ç®€ä»‹ / é£ŽæŽ§å›¢é˜ŸèŒè´£ / äº¤æ˜“ç³»ç»Ÿæž¶æž„â€¦\", scale=6)\n",
    "        send = gr.Button(\"å‘é€\", variant=\"primary\", scale=1)\n",
    "        clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "    with gr.Accordion(\"é«˜çº§è®¾ç½®\", open=False):\n",
    "        temperature = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label=\"ç”Ÿæˆæ¸©åº¦\")\n",
    "        n_ctx = gr.Slider(3, 30, value=20, step=1, label=\"å¬å›žæ¡æ•°\")\n",
    "        index_name = gr.Textbox(value=DEFAULT_INDEX, label=\"ES ç´¢å¼•å\")\n",
    "\n",
    "    def _submit(user_msg, chat_hist, temperature, n_ctx, index_name):\n",
    "        return respond(user_msg, chat_hist, temperature, int(n_ctx), index_name)\n",
    "\n",
    "    send.click(_submit, [msg, chatbot, temperature, n_ctx, index_name], [chatbot, hits, dbg]) \\\n",
    "        .then(lambda: \"\", None, msg)\n",
    "    msg.submit(_submit, [msg, chatbot, temperature, n_ctx, index_name], [chatbot, hits, dbg]) \\\n",
    "        .then(lambda: \"\", None, msg)\n",
    "    clear.click(lambda: ([], \"ï¼ˆå·²æ¸…ç©ºï¼‰\", \"â€”\"), None, [chatbot, hits, dbg])\n",
    "\n",
    "# ä½œä¸ºè„šæœ¬å¯åŠ¨\n",
    "if __name__ == \"__main__\":\n",
    "    # å¦‚æžœéœ€è¦å…¬ç½‘è®¿é—®å¯ä¼  share=Trueï¼›æœåŠ¡å™¨çŽ¯å¢ƒå¯æ”¹ server_name=\"0.0.0.0\"\n",
    "    demo.queue().launch(server_name=\"127.0.0.1\", server_port=7861, share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9655ce-96cc-4f08-b271-b0af81a13a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb7867-f2c1-4b8c-ad5b-f3f498077060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb87d52-5c97-443d-8086-3f3ae2d76e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b5c1f-1058-4061-9e64-f8137a7a30f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5be82c09-2938-4a41-9c95-3ed1a4349c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text = \"ä¸»è¦ç­–ç•¥ä»‹ç»\"\n",
    "q_vec = embed([q_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4042e35e-6e6d-456d-a74f-2b1da16d9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text = \"å¾çŽ®\"\n",
    "\n",
    "# æ–¹å¼Aï¼šé€šç”¨ encodeï¼ˆè®°å¾— is_query=Trueï¼‰\n",
    "out = model.encode([q_text])\n",
    "q_vec = out[\"dense_vecs\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c23d34e-66d2-45df-a402-b46afcc303f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_query(q_text: str, q_vec: list[float], dense_weight: float = 1.0):\n",
    "    # é’ˆå¯¹çŸ­è¯ï¼ˆ<=4 ä¸ªæ±‰å­—ï¼‰æŠŠç¨€ç–æƒé‡è°ƒé«˜ã€å‘é‡æƒé‡è°ƒä½Ž\n",
    "    is_short = len(q_text) <= 4 and all('\\u4e00' <= ch <= '\\u9fff' for ch in q_text)\n",
    "    sparse_boost = 4.0 if is_short else 1.5\n",
    "    dense_w = 0.5 if is_short else dense_weight\n",
    "\n",
    "    base_bool = {\n",
    "        \"should\": [\n",
    "            {   # BM25 è¦†ç›–æ­£æ–‡\n",
    "                \"multi_match\": {\n",
    "                    \"query\": q_text,\n",
    "                    \"fields\": [\"chunk^1\", \"section_text^5\"],\n",
    "                    \"type\": \"most_fields\"\n",
    "                }\n",
    "            },\n",
    "            {   # ç²¾ç¡®å‘½ä¸­ sectionï¼ˆå¦‚â€œå…¬å¸ç®€ä»‹â€ï¼‰å¼ºåŠ›åŠ æƒ\n",
    "                \"term\": {\"section\": {\"value\": q_text, \"boost\": sparse_boost}}\n",
    "            },\n",
    "            {   # è¯ç»„å‘½ä¸­ä¹ŸåŠ æƒ\n",
    "                \"match_phrase\": {\"chunk\": {\"query\": q_text, \"boost\": 3.0}}\n",
    "            }\n",
    "        ],\n",
    "        \"minimum_should_match\": 1\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"function_score\": {\n",
    "            \"query\": {\"bool\": base_bool},\n",
    "            \"boost_mode\": \"sum\",\n",
    "            \"score_mode\": \"sum\",\n",
    "            \"functions\": [\n",
    "                {   # å‘é‡åˆ†\n",
    "                    \"script_score\": {\n",
    "                        \"script\": {\n",
    "                            \"source\": \"cosineSimilarity(params.qv, doc['chunk_vector']) + 1.0\",\n",
    "                            \"params\": {\"qv\": q_vec}\n",
    "                        }\n",
    "                    },\n",
    "                    \"weight\": dense_w\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b759553-8b70-4e76-b313-c7a8ad77bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = es.search(\n",
    "    index=\"dd_report_data\",\n",
    "    size=20,\n",
    "    query=hybrid_query(q_text, q_vec),\n",
    "    _source=[\"doc_id\",\"block_id\",\"part_id\",\"section\",\"chunk\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2923c3c7-98b1-4608-8124-7d881e0fd15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [65.3692] \n",
      "ðŸ“„ - **å…¬å¸ä¸»è¦æŠ•èµ„ç­–ç•¥**ï¼š1.é‡åŒ–è‚¡ç¥¨ç­–ç•¥ï¼š\n",
      "ï¼ˆ1ï¼‰æŒ‡æ•°å¢žå¼ºç­–ç•¥ï¼šåŸºé‡‘åœ¨è¿›è¡ŒæŒ‡æ•°åŒ–æŠ•èµ„çš„è¿‡ç¨‹ä¸­ï¼Œä¸ºè¯•å›¾èŽ·å¾—è¶…è¶ŠæŒ‡æ•°çš„æŠ•èµ„å›žæŠ¥ï¼Œåœ¨è¢«åŠ¨è·Ÿè¸ªæŒ‡æ•°çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å¢žå¼ºåž‹çš„ç§¯æžæŠ•èµ„æ‰‹æ®µï¼ˆå¯¹ä¸ªè‚¡ã€è¡Œä¸šã€é£Žæ ¼è¿›è¡Œé€‚å½“è°ƒèŠ‚ï¼‰ï¼Œå¯¹æŠ•èµ„ç»„åˆä¸­æŒè‚¡çš„è¡Œä¸šè½®åŠ¨åŠä¸ªè‚¡å¢žå¼ºè¿›è¡Œé€‚å½“è°ƒæ•´ï¼ŒåŠ›æ±‚åœ¨åˆ©ç”¨é‡åŒ–æ¨¡åž‹æŽ§åˆ¶é£Žé™©çš„åŒæ—¶èŽ·å–ç§¯æžçš„å¸‚åœºæ”¶ç›Šã€‚ç›®å‰å®½æŠ•çš„æŒ‡å¢žäº§å“çº¿å·²è¦†ç›–ä¸­è¯500ã€ä¸­è¯1000ã€æ²ªæ·±300ç­‰ä¸»æµæŒ‡æ•°ã€‚\n",
      "ï¼ˆ2ï¼‰é‡åŒ–é€‰è‚¡ç­–ç•¥ï¼šä¸»è¦åˆ©ç”¨æ•°å­¦æ¨¡åž‹å’Œç®—æ³•ç­›é€‰è‚¡ç¥¨ã€æž„å»ºæŠ•èµ„ç»„åˆï¼Œä»¥åšå¤šä¸ºä¸»å¹¶è¿½æ±‚è¶…é¢æ”¶ç›Šï¼ˆAlphaï¼‰çš„ç­–ç•¥ï¼Œæ ¸å¿ƒæ˜¯é€šè¿‡æ•°æ®åˆ†æžå’Œç»Ÿè®¡æ¨¡åž‹æ•æ‰å¸‚åœºä¸­çš„è§„å¾‹æ€§æœºä¼šã€‚é€‰è‚¡èŒƒå›´å¹¶ä¸æ‹˜æŸäºŽæŸä¸€æŒ‡æ•°ï¼ŒæŠ•èµ„ç»„åˆä¹Ÿä¸é’ˆå¯¹...\n",
      "------------------------------------------------------------\n",
      "âœ… [48.7821] \n",
      "ðŸ“„ - **å…¬å¸ç­–ç•¥æœ‰æ•ˆ/å¤±æ•ˆçš„ä¸»è¦å½±å“å› ç´ **ï¼š1ã€å¸‚åœºç»“æž„ï¼šé‡åŒ–æ¨¡åž‹åŸºäºŽåŽ†å²æ•°æ®æž„å»ºï¼Œåœ¨å¸‚åœºç»“æž„ç¨³å®šæ—¶å¯æ­£å¸¸å‘æŒ¥ï¼Œè‹¥å¸‚åœºç»“æž„çªå˜ï¼ˆå¦‚æ”¿ç­–å†²å‡»ã€é»‘å¤©é¹…äº‹ä»¶ï¼‰ï¼Œç­–ç•¥å¯èƒ½è¡¨çŽ°ä¸ä½³ã€‚\n",
      "2ã€å¸‚åœºæœ‰æ•ˆæ€§ï¼šæ–°å…´å¸‚åœºæˆ–æ•£æˆ·å æ¯”è¾ƒé«˜çš„å¸‚åœºï¼ˆå¦‚Aè‚¡ï¼‰ï¼Œä¿¡æ¯ä¸å¯¹ç§°æœºä¼šå¤šï¼ŒAlphaæ˜“èŽ·å–ã€‚è‹¥ç«žäº‰åŠ å‰§ï¼Œå¤šå®¶æœºæž„ä½¿ç”¨ç›¸ä¼¼å› å­ï¼ˆå¦‚åŠ¨é‡ã€ä¼°å€¼ï¼‰ï¼Œæ˜“å¯¼è‡´ç­–ç•¥æ‹¥æŒ¤ï¼Œæ”¶ç›Šè¢«æ‘Šè–„ã€‚\n",
      "3ã€å¸‚åœºæµåŠ¨æ€§ä¸Žæ³¢åŠ¨æ€§ï¼šé«˜æµåŠ¨æ€§å¸‚åœºä¸”æ­£å¸¸æ³¢åŠ¨çŽ¯å¢ƒï¼Œä¾¿äºŽç­–ç•¥å¿«é€Ÿè°ƒä»“ï¼Œå‡å°‘æ»‘ç‚¹æŸè€—ï¼Œé‡ä»·å› å­æœ‰æ•ˆæ€§è¾ƒé«˜ã€‚å½“å¸‚åœºå‰§çƒˆæ³¢åŠ¨æ—¶å™ªå£°å¹²æ‰°å¢žåŠ ï¼Œæ¨¡åž‹å¯èƒ½è¿‡æ‹Ÿåˆã€‚ç‰¹åˆ«æ˜¯åœ¨å•è¾¹æš´è·Œæˆ–æš´æ¶¨è¡Œæƒ…ä¸­ï¼Œé‡åŒ–æ¨¡åž‹å¯èƒ½æ— æ³•æœ‰æ•ˆåº”å¯¹éžçº¿æ€§æ³¢åŠ¨ï¼ˆå¦‚æµåŠ¨æ€§æž¯ç«­ï¼‰ã€‚\n",
      "4ã€æ¨¡åž‹å¤æ‚ç¨‹åº¦ï¼š...\n",
      "------------------------------------------------------------\n",
      "âœ… [36.0890] \n",
      "ðŸ“„ - **å­å…¬å¸åŠä¸»è¦å…³è”æ–¹**ï¼šæ— ...\n",
      "------------------------------------------------------------\n",
      "âœ… [34.2861] \n",
      "ðŸ“„ - **å…¬å¸ç®€ä»‹**ï¼šä¸Šæµ·å®½æŠ•èµ„äº§ç®¡ç†æœ‰é™å…¬å¸æˆç«‹äºŽ2014å¹´12æœˆï¼Œ2015å¹´4æœˆé€šè¿‡ä¸­å›½è¯åˆ¸æŠ•èµ„åŸºé‡‘ä¸šåä¼šå¤‡æ¡ˆï¼Œ2017å¹´4æœˆæˆä¸ºåä¼šä¼šå‘˜å•ä½ï¼Œ2018å¹´4æœˆèŽ·å¾—æŠ•é¡¾èµ„æ ¼ï¼Œ2021å¹´èµ„äº§ç®¡ç†è§„æ¨¡çªç ´100äº¿å…ƒã€‚å…¬å¸ä¸“æ³¨äºŽå›½å†…äºŒçº§å¸‚åœºé‡åŒ–æŠ•èµ„äº¤æ˜“ï¼ŒæŠ•ç ”å›¢é˜Ÿç”±å…·å¤‡å›½å†…å¤–çŸ¥åé«˜æ ¡ç»Ÿè®¡å­¦ã€æ•°å­¦ã€é‡‘èžå·¥ç¨‹ã€è®¡ç®—æœºç§‘å­¦ç­‰ä¸“ä¸šçš„ç¡•å£«æˆ–åšå£«å­¦ä½çš„é¡¶å°–é‡åŒ–ç ”ç©¶äººæ‰æž„æˆï¼Œæ˜¯å›½å†…æœ€æ—©æ¶‰è¶³é‡åŒ–æŠ•èµ„çš„å›¢é˜Ÿä¹‹ä¸€ï¼Œå¯¹äºŽé‡åŒ–åˆ†æžæ–¹æ³•å’Œé‡åŒ–æŠ•èµ„ç­–ç•¥çš„åº”ç”¨å…·å¤‡ä¸°å¯Œçš„å®žç›˜ç»éªŒï¼Œè‡´åŠ›äºŽä¸ºæŠ•èµ„è€…æŒç»­åˆ›é€ ç¨³å¥çš„æŠ•èµ„æ”¶ç›Šã€‚å…¬å¸äº§å“çº¿åŒ…æ‹¬æŒ‡æ•°å¢žå¼ºã€é‡åŒ–é€‰è‚¡ã€æœŸè´§CTAç­‰å¤šç­–ç•¥ä½“ç³»ï¼Œåˆä½œæœºæž„è¦†ç›–å›½å†…ä¸»æµåˆ¸å•†ã€é“¶è¡Œã€ä¿¡æ‰˜ã€æœŸè´§ç­‰å„ç±»é‡‘èžæœºæž„...\n",
      "------------------------------------------------------------\n",
      "âœ… [32.9876] \n",
      "ðŸ“„ - **ç§‘åˆ›ç²¾é€‰ç­–ç•¥ç‰¹ç‚¹**ï¼šé‡åŒ–é€‰è‚¡ç­–ç•¥ä¸»è¦åœ¨å®½åŸºæŒ‡æ•°èŒƒå›´å†…é€‰è‚¡ï¼Œå¹¶å‰”é™¤ST/*STç­‰ã€ä»¥åŠä¸Šå¸‚ä¸è¶³1å¹´çš„è‚¡ç¥¨ä¸ºé€‰è‚¡æ± ã€‚æŒè‚¡è¾ƒä¸ºåˆ†æ•£ï¼ŒæŒè‚¡æ•°é‡ä¸€èˆ¬åœ¨æ•°ç™¾è‡³ä¸Šåƒåªï¼Œå•ç¥¨ä¸Šé™ä¸€èˆ¬ä¸è¶…è¿‡1%ï¼›æ³¢åŠ¨çŽ‡ä¸è¶…è¿‡30%ï¼›å•ä¸€è¡Œä¸šé›†ä¸­åº¦ä¸è¶…è¿‡20%ã€‚æ ¹æ®ç»†åˆ†ç­–ç•¥ä¸åŒä¼šæœ‰ç›¸åº”ä¾§é‡å’Œè°ƒæ•´ï¼Œå¦‚ç§‘åˆ›ç²¾é€‰ç­–ç•¥ä¸»è¦æ˜¯åœ¨åœ¨ç§‘åˆ›æ¿å—èŒƒå›´å†…é€‰è‚¡ï¼ŒåŒæ—¶ä¼šé’ˆå¯¹ç§‘åˆ›æ¿å—ç›¸å¯¹ç‰¹æ®Šçš„ç»“æž„ç‰¹å¾ï¼Œé€‚å½“æ”¾å®½å¯¹äºŽä¸ªè‚¡é›†ä¸­åº¦å’Œè¡Œä¸šé›†ä¸­åº¦çš„è¦æ±‚ã€‚åœ¨å› å­å±‚é¢ï¼Œåœ¨åŸºæœ¬é¢éƒ¨åˆ†ç­–ç•¥å°è¯•åŠ å…¥æ›´å¤šå·²ç»åœ¨å…¨åŸŸèŒƒå›´å†…æœ‰è¾ƒå¥½å¯è§£é‡Šæ€§å’Œæ£€éªŒæ•ˆæžœçš„å¹¶ä¸”åœ¨ç§‘åˆ›æ¿å—å†…ä¹Ÿæœ‰è¾ƒå¥½åº”ç”¨æ€§çš„å› å­ã€‚åœ¨é‡ä»·éƒ¨åˆ†ï¼ŒåŸºäºŽé‡ä»·ç±»å› å­åœ¨è¿‡åŽ»è¡¨çŽ°ç›¸å¯¹ç¨³å®šï¼Œç­–ç•¥ä¼šé’ˆå¯¹æ€§å¢žåŠ é«˜æ³¢åŠ¨ç±»ã€é•¿åŠ¨é‡ç±»çš„å› ...\n",
      "------------------------------------------------------------\n",
      "âœ… [31.3202] \n",
      "ðŸ“„ - **ç§‘åˆ›ç­–ç•¥å¸‚åœºè§‚ç‚¹**ï¼šè‡ªåŽ»å¹´9æœˆä»½å¸‚åœºåå¼¹ä»¥æ¥ï¼Œå¸‚åœºçš„é«˜æ´»è·ƒåº¦ä»¥åŠé«˜æˆäº¤é‡å¯¹äºŽè¶…é¢æ”¶ç›Šçš„æ”¯æ’‘ä½œç”¨ååˆ†æ˜¾è‘—ã€‚å°±å½“å‰é˜¶æ®µè€Œè¨€ï¼Œå¼ºåŠ¿çš„Betaä»¥åŠç¨³å®šçš„Alphaå‡è¡¨æ˜Žé‡åŒ–ç­–ç•¥å…·å¤‡æžé«˜çš„é…ç½®æ€§ä»·æ¯”ã€‚éšç€å¤§æ•°æ®ã€AIäººå·¥æ™ºèƒ½ç­‰å‰æ²¿ç§‘æŠ€çš„é£žé€Ÿå‘å±•ï¼Œç­–ç•¥çš„æž„å»ºå°†æ›´åŠ ç²¾å‡†é«˜æ•ˆï¼Œæµ·é‡æ•°æ®çš„æ·±åº¦æŒ–æŽ˜ä¸Žåˆ†æžï¼Œä¸ºæé«˜ç­–ç•¥æ”¶ç›Šæä¾›äº†åšå®žçš„åŸºç¡€ã€‚\n",
      "ä»Žæ¿å—å±‚é¢æ¥çœ‹ï¼Œæ”¿åºœæ”¿ç­–å‚¬åŒ–å¤§éƒ½é›†ä¸­åœ¨ç§‘æŠ€æ¿å—ï¼Œç§‘åˆ›æ¿å—æ”¿ç­–æŽ¨åŠ¨ï¼Œç›¸æ¯”å…¶ä»–æ¿å—æœ‰æ›´é«˜çš„ä¸Šæ¶¨åŠ¨èƒ½ï¼›åŒæ—¶ç›¸æ¯”äºŽå®½åŸºæŒ‡æ•°è·ç¦»21å¹´é«˜ç‚¹åˆ†ä½ï¼Œç§‘åˆ›æ¿æœ‰æ›´é«˜çš„æ¶¨å¹…æ¯”ä¾‹ç©ºé—´ã€‚ä»Žèµ„é‡‘æ€§è´¨æ¥çœ‹ï¼Œæœ¬è½®æ–°å¢žèµ„é‡‘ä¸»è¦æ¥æºä¸ºä¸ªäººæŠ•èµ„è€…ï¼Œå½“å‰ç§‘åˆ›æ¿çš„æœºæž„æŠ•èµ„è€…å æ¯”ä¸è¶³20%ï¼Œåœ¨æœ¬è½®ä¸ªäººå®¢æˆ·æ–°å¢ž...\n",
      "------------------------------------------------------------\n",
      "âœ… [29.1721] \n",
      "ðŸ“„ - **å…¬å¸é‡åŒ–ç­–ç•¥ä¼˜ç¼ºç‚¹**ï¼šä¼˜ç‚¹ï¼š\n",
      "1ã€çºªå¾‹æ€§ä¸Žå®¢è§‚æ€§ï¼šä¾èµ–é¢„è®¾çš„è§„åˆ™å’Œç®—æ³•ï¼Œé¿å…ä¸»è§‚æƒ…ç»ªå¹²æ‰°ï¼Œå‡å°‘äººä¸ºåˆ¤æ–­å¤±è¯¯ã€‚ä¸¥æ ¼æ‰§è¡Œæ­¢ç›ˆæ­¢æŸï¼Œé™ä½Žéžç†æ€§äº¤æ˜“é£Žé™©ã€‚\n",
      "2ã€æ•°æ®å¤„ç†èƒ½åŠ›ï¼šå¯å¿«é€Ÿåˆ†æžæµ·é‡æ•°æ®ï¼ˆè´¢åŠ¡æ•°æ®ã€ä»·é‡æŒ‡æ ‡ã€èˆ†æƒ…ç­‰ï¼‰ï¼ŒæŒ–æŽ˜äººè„‘éš¾ä»¥è¯†åˆ«çš„çŸ­æœŸè§„å¾‹ã€‚\n",
      "3ã€é£Žé™©åˆ†æ•£ï¼šé€šå¸¸æŒä»“åˆ†æ•£ï¼ˆæ•°ç™¾è‡³ä¸Šåƒåªè‚¡ç¥¨ï¼‰ï¼Œé™ä½Žå•ä¸€æ ‡çš„æˆ–è¡Œä¸šçš„é£Žé™©æš´éœ²ã€‚\n",
      "4ã€é€‚åº”æ€§å¼ºï¼šé«˜é¢‘æˆ–ä¸­ä½Žé¢‘ç­–ç•¥å‡èƒ½çµæ´»è°ƒæ•´å‚æ•°ï¼Œé€‚åº”å¸‚åœºé£Žæ ¼åˆ‡æ¢ï¼ˆå¦‚æˆé•¿/ä»·å€¼è½®åŠ¨ï¼‰ã€‚\n",
      "5ã€ä½Žäº¤æ˜“æˆæœ¬ï¼šç®—æ³•ä¼˜åŒ–äº¤æ˜“æ‰§è¡Œè·¯å¾„ï¼ˆå¦‚TWAPã€VWAPï¼‰ï¼Œå‡å°‘å†²å‡»æˆæœ¬ã€‚\n",
      "ç¼ºç‚¹ï¼š\n",
      "1ã€åŽ†å²æ•°æ®ä¾èµ–ï¼šæ¨¡åž‹åŸºäºŽåŽ†å²è§„å¾‹æž„å»ºï¼Œè‹¥å¸‚åœºç»“æž„çªå˜ï¼ˆå¦‚æ”¿ç­–å†²å‡»ã€é»‘å¤©é¹…äº‹ä»¶ï¼‰ï¼Œç­–ç•¥å¯èƒ½...\n",
      "------------------------------------------------------------\n",
      "âœ… [27.7090] \n",
      "ðŸ“„ - **å…¬å¸ç­–ç•¥å®¹é‡è®¡ç®—æ–¹æ³•**ï¼šé‡åŒ–è‚¡ç¥¨ç­–ç•¥çš„å®¹é‡åœ¨200äº¿å·¦å³ï¼Œä»¥20250228æˆäº¤é‡ä¸ºä¾‹ï¼Œä¸Šè¯æŒ‡æ•°äº¤æ˜“é‡7581äº¿ï¼Œè‚¡ç¥¨æ•°2249ï¼Œå•åªè‚¡ç¥¨äº¤æ˜“é‡çº¦ä¸º3.37äº¿ï¼›æ·±è¯ç»¼æŒ‡äº¤æ˜“é‡1.11ä¸‡äº¿ï¼Œè‚¡ç¥¨æ•°2892ï¼Œå•åªè‚¡ç¥¨äº¤æ˜“é‡3.83äº¿ã€‚åˆ›ä¸šæ¿æŒ‡æ•°æˆäº¤é‡‘é¢5309äº¿ï¼Œè‚¡ç¥¨æ•°1369ï¼Œå•åªè‚¡ç¥¨äº¤æ˜“é‡çº¦ä¸º3.88äº¿ã€‚ç»¼ä¸Šï¼Œå…¨å¸‚åœºå•åªè‚¡ç¥¨å•æ—¥å¸‚åœºæˆäº¤é‡å¹³å‡çº¦3.70äº¿ï¼Œæ— å¸‚åœºå†²å‡»äº¤æ˜“å æ¯”0.5-1%ï¼Œå•åªè‚¡ç¥¨æ— å¸‚åœºå†²å‡»æˆäº¤é‡ä¸º185-370ä¸‡ã€‚é‡åŒ–è‚¡ç¥¨ç­–ç•¥æŒè‚¡æ•°ä¸€èˆ¬çº¦1000åªå·¦å³ï¼Œæ¢æ‰‹çŽ‡å¹´åŒ–åŒè¾¹50-80å€ï¼ˆçº¦æ¯æ—¥åŒè¾¹20%-30%ï¼‰ï¼Œä»¥1000åªè‚¡ç¥¨ï¼Œæ¢æ‰‹çŽ‡20%ï¼Œæµ‹ç®—ç­–ç•¥å®¹é‡=370ä¸‡*10...\n",
      "------------------------------------------------------------\n",
      "âœ… [24.8100] \n",
      "ðŸ“„ - **å…¬å¸äº§å“æ˜¯å¦åŒç­–ç•¥ï¼Œèƒ½å¦å®šåˆ¶**ï¼šå…¬å¸çš„ç­–ç•¥è¾ƒä¸°å¯Œï¼Œä¸æ˜¯æ¯ä¸ªäº§å“éƒ½é‡‡ç”¨åŒæ ·çš„æŠ•èµ„ç­–ç•¥ï¼Œå¯ä»¥æ ¹æ®å®¢æˆ·éœ€æ±‚è¿›è¡Œå¤åˆç­–ç•¥äº§å“çš„å®šåˆ¶ï¼Œä½†æœ‰ä¸€å®šçš„æŠ•èµ„é—¨æ§›ã€‚...\n",
      "------------------------------------------------------------\n",
      "âœ… [23.2557] \n",
      "ðŸ“„ - **å…¬å¸ä¸»é¡µ**ï¼šwww.quantinv.com...\n",
      "------------------------------------------------------------\n",
      "âœ… [21.6775] \n",
      "ðŸ“„ ä¸»è¦ç­–ç•¥ä»‹ç»ï¼š1.é‡åŒ–è‚¡ç¥¨ç­–ç•¥ï¼šä¸»è¦åˆ©ç”¨æ•°å­¦æ¨¡åž‹å’Œç®—æ³•ç­›é€‰è‚¡ç¥¨ã€æž„å»ºæŠ•èµ„ç»„åˆï¼Œä»¥åšå¤šä¸ºä¸»å¹¶è¿½æ±‚è¶…é¢æ”¶ç›Šï¼ˆAlphaï¼‰çš„ç­–ç•¥ï¼Œæ ¸å¿ƒæ˜¯é€šè¿‡æ•°æ®åˆ†æžå’Œç»Ÿè®¡æ¨¡åž‹æ•æ‰å¸‚åœºä¸­çš„è§„å¾‹æ€§æœºä¼šã€‚å…·ä½“è€Œè¨€ï¼ŒåŒ…æ‹¬æŒ‡æ•°å¢žå¼ºç­–ç•¥ã€é‡åŒ–é€‰è‚¡ç­–ç•¥ç­‰ã€‚\n",
      "ï¼ˆ1ï¼‰æŒ‡æ•°å¢žå¼ºç­–ç•¥æ˜¯åŸºé‡‘åœ¨è¿›è¡ŒæŒ‡æ•°åŒ–æŠ•èµ„çš„è¿‡ç¨‹ä¸­ï¼Œä¸ºè¯•å›¾èŽ·å¾—è¶…è¶ŠæŒ‡æ•°çš„æŠ•èµ„å›žæŠ¥ï¼Œåœ¨è¢«åŠ¨è·Ÿè¸ªæŒ‡æ•°çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥å¢žå¼ºåž‹çš„ç§¯æžæŠ•èµ„æ‰‹æ®µï¼ˆå¯¹ä¸ªè‚¡ã€è¡Œä¸šã€é£Žæ ¼è¿›è¡Œé€‚å½“è°ƒèŠ‚ï¼‰ï¼Œå¯¹æŠ•èµ„ç»„åˆä¸­æŒè‚¡çš„è¡Œä¸šè½®åŠ¨åŠä¸ªè‚¡å¢žå¼ºè¿›è¡Œé€‚å½“è°ƒæ•´ï¼ŒåŠ›æ±‚åœ¨åˆ©ç”¨é‡åŒ–æ¨¡åž‹æŽ§åˆ¶é£Žé™©çš„åŒæ—¶èŽ·å–ç§¯æžçš„å¸‚åœºæ”¶ç›Šã€‚ç›®å‰å®½æŠ•çš„æŒ‡å¢žäº§å“çº¿æ—¢è¦†ç›–ä¸­è¯500ã€ä¸­è¯1000ç­‰ä¸»æµæŒ‡æ•°ï¼Œä¹ŸåŒ…æ‹¬çº¢åˆ©æŒ‡æ•°ï¼Œæœºå™¨äººæŒ‡æ•°ç­‰å®šåˆ¶åŒ–æŒ‡å¢žäº§å“ã€‚\n",
      "å…¶ä¸­...\n",
      "------------------------------------------------------------\n",
      "âœ… [19.2353] \n",
      "ðŸ“„ - **å…¬å¸æŠ•èµ„å†³ç­–æµç¨‹**ï¼š1.æ•°æ®æ”¶é›†ä¸Žæ•´ç†\n",
      "ï¼ˆ1ï¼‰åŸºæœ¬é¢æ•°æ®ï¼šä¼ ç»ŸåŸºæœ¬é¢ç±»ï¼ˆè´¢æŠ¥ï¼‰ã€å…¬å‘Šç±»ã€åˆ†æžå¸ˆä¸€è‡´é¢„æœŸç­‰ã€‚ä¸»è¦ç‰¹ç‚¹ä¸ºç¦»æ•£åŒ–æ•°æ®ï¼Œå…·æœ‰è¾ƒå¼ºçš„è§£é‡Šæ€§ã€‚\n",
      "ï¼ˆ2ï¼‰é‡ä»·æ•°æ®ï¼šåˆ¸å•†Level IIè¡Œæƒ…æ•°æ®ä¸ºä¸»ï¼ŒåŒ…æ‹¬é€ç¬”æˆäº¤æ•°æ®ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹ä¸ºæ—¶é—´ä»¥åŠåˆ¶å¼å…¨éƒ¨æ ‡å‡†åŒ–ï¼Œéœ€è¦å¯¹æ•°æ®çš„é€»è¾‘åŠäº¤æ˜“è¡Œä¸ºæœ‰è®¤çŸ¥å’Œç†è§£ã€‚\n",
      "ï¼ˆ3ï¼‰å¦ç±»æ•°æ®ï¼šæŒä»“æ•°æ®ï¼Œæƒ…ç»ªæ•°æ®ã€æ–°é—»èˆ†æƒ…ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹ä¸ºä¸ªæ€§åŒ–å¼ºï¼Œé€æ˜Žåº¦ä½Žï¼Œéœ€è¦æ ‡å‡†åŒ–å¤„ç†ã€‚\n",
      "2.å› å­æŒ–æŽ˜ä¸Žè¯„ä»·\n",
      "ï¼ˆ1ï¼‰ç‰¹å¾å’Œæ ‡ç­¾æå–ï¼šæ ¹æ®æ•´ç†åŽçš„æ•°æ®ï¼Œè¿›è¡Œæ ‡ç­¾åˆ¶ä½œå’Œç‰¹å¾æž„å»ºã€‚\n",
      "ï¼ˆ2ï¼‰æ¨¡åž‹è®­ç»ƒ/å›žæµ‹æ£€éªŒï¼šå‘æ¨¡åž‹å±•ç¤ºæœ‰æ ‡ç­¾æ ·æœ¬è¿›è¡Œæ»šåŠ¨æ¨¡åž‹è®­ç»ƒï¼Œæ ·æœ¬å†…è®­ç»ƒåŽè¿›è¡Œäº¤å‰éªŒè¯å¹¶è°ƒæ•´å‚æ•°ï¼Œå…³æ³¨æ•°æ®åŠæ¨¡åž‹...\n",
      "------------------------------------------------------------\n",
      "âœ… [17.4623] \n",
      "ðŸ“„ - **æŠ•å†³ä¼šçš„å»ºåˆ¶å’Œå†³ç­–æœºåˆ¶**ï¼šæŠ•å†³ä¼šçš„ç»„æˆäººå‘˜ä¸ºå…¬å¸æ ¸å¿ƒç­–ç•¥åˆåˆ›å›¢é˜Ÿï¼ŒåŒ…æ‹¬é’±æˆã€ä¸¥å®“åŠéƒ‘éš†ã€‚è´Ÿè´£åŒ…æ‹¬ç­–ç•¥ç ”å‘å…¨æµç¨‹çš„æŠ•å†³äº‹é¡¹ã€‚\n",
      "æŠ•å†³æœºåˆ¶ä¸»è¦ä¸ºï¼ŒåŸºäºŽç­–ç•¥ç ”å‘å…¨æµç¨‹æµæ°´çº¿èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬å…¥åº“ã€è¿­ä»£åŠå‰”é™¤ç­‰ï¼‰è¿›è¡Œå†³ç­–ç ”è®¨åŠåˆ¶å®šï¼Œé’ˆå¯¹æ¯ä¸ªèŠ‚ç‚¹åˆ¶å®šå¯¹åº”çš„æŒ‡æ ‡æ ‡å‡†ï¼ŒåŒæ—¶å°†æ•°æ®æ£€éªŒç»“åˆç»éªŒæ£€éªŒï¼Œä»¥ä¿è¯ç­–ç•¥ç ”å‘çš„æŒç»­æ€§ä¸Žè¿­ä»£ã€‚...\n",
      "------------------------------------------------------------\n",
      "âœ… [10.6663] \n",
      "ðŸ“„ - **å…¬å¸æŠ•ç ”å›¢é˜Ÿäººå‘˜**ï¼šæŠ•ç ”äººå‘˜æ•°é‡ï¼š25äºº\n",
      "æŠ•ç ”å›¢é˜Ÿå¹³å‡ä»Žä¸šå¹´é™ï¼š7å¹´ä»¥ä¸Š\n",
      "æŠ•ç ”äººå‘˜å…·ä½“ä»‹ç»ï¼š\n",
      "æŠ•ç ”å›¢é˜Ÿç”±å…·å¤‡æµ·å†…å¤–çŸ¥åé«˜æ ¡ç»Ÿè®¡å­¦ã€æ•°å­¦ã€é‡‘èžå·¥ç¨‹ä¸Žè®¡ç®—æœºç§‘å­¦ç­‰ä¸“ä¸šçš„ç¡•å£«æˆ–åšå£«å­¦ä½çš„é¡¶å°–é‡åŒ–ç ”ç©¶äººæ‰æž„æˆï¼Œæ˜¯å›½å†…æœ€æ—©æ¶‰è¶³é‡åŒ–æŠ•èµ„çš„å›¢é˜Ÿä¹‹ä¸€ï¼Œå¯¹äºŽé‡åŒ–åˆ†æžæ–¹æ³•å’Œé‡åŒ–æŠ•èµ„ç­–ç•¥çš„åº”ç”¨å…·å¤‡ä¸°å¯Œçš„å®žç›˜ç»éªŒã€‚\n",
      "\n",
      "æŠ•ç ”å›¢é˜Ÿåˆ†å·¥ï¼š\n",
      "1ï¼‰è‚¡ç¥¨é‡åŒ–å›¢é˜Ÿï¼š11äººï¼Œè´Ÿè´£ALPHAå¤šå› å­ç­–ç•¥ç ”å‘ï¼Œå¹³å‡ä»Žä¸šå¹´é™çº¦8å¹´ï¼›\n",
      "2ï¼‰æœŸè´§é‡åŒ–å›¢é˜Ÿï¼š5äººï¼Œè´Ÿè´£CTAç­–ç•¥ç ”å‘ï¼Œå¹³å‡ä»Žä¸šå¹´é™çº¦7å¹´ï¼›\n",
      "3ï¼‰ç³»ç»Ÿç ”å‘å›¢é˜Ÿï¼š6äººï¼Œè´Ÿè´£äº¤æ˜“åŠé£ŽæŽ§ç³»ç»Ÿæ­å»ºï¼Œå¹³å‡ä»Žä¸šå¹´é™çº¦8å¹´ï¼›\n",
      "4ï¼‰äº¤æ˜“å›¢é˜Ÿï¼š3äººï¼Œè´Ÿè´£ç®—æ³•äº¤æ˜“ç³»ç»Ÿè¿è¡Œç»´æŠ¤ï¼Œå¹³å‡ä»Žä¸šå¹´é™çº¦6å¹´...\n",
      "------------------------------------------------------------\n",
      "âœ… [9.4491] \n",
      "ðŸ“„ - **å…¬å¸ä¿¡æ¯å®‰å…¨**ï¼šä¿¡æ¯å®‰å…¨ï¼š\n",
      "ï¼ˆ1ï¼‰æ•°æ®åŠ å¯†ï¼šå¯¹æ•æ„Ÿæ•°æ®è¿›è¡ŒåŠ å¯†å¤„ç†ï¼Œæ— è®ºæ˜¯å­˜å‚¨è¿˜æ˜¯ä¼ è¾“è¿‡ç¨‹ä¸­ã€‚å¿…è¦çš„ç­–ç•¥åç§°è¿›è¡Œè„±æ•ã€‚\n",
      "ï¼ˆ2ï¼‰æ“ä½œç•™ç—•ï¼šç­–ç•¥ç”Ÿäº§ä»£ç ï¼ŒåŸºäºŽä»£ç ç‰ˆæœ¬æŽ§åˆ¶ç³»ç»Ÿï¼Œæ‰€æœ‰çš„æ”¹åŠ¨å‡æœ‰ç•™ç—•ã€‚ä¸”æ¯æ¬¡æ”¹åŠ¨å‡éœ€è¦å¤šäººç¡®è®¤æ‰èƒ½æäº¤è‡³ç”Ÿäº§çŽ¯å¢ƒã€‚\n",
      "ï¼ˆ3ï¼‰è‡ªåŠ¨åŒ–æ“ä½œï¼šè‡ªåŠ¨æ›´æ–°ä»£ç ï¼Œå¹¶åœ¨ç³»ç»Ÿå†…è¿è¡Œã€‚å‡å°‘äººå·¥ä»‹å…¥çš„å¯èƒ½æ€§ã€‚...\n",
      "------------------------------------------------------------\n",
      "âœ… [7.8643] \n",
      "ðŸ“„ - **äººå‘˜åˆ†å¸ƒ**ï¼š\n",
      "éƒ¨é—¨åç§°ï¼šç­–ç•¥äº¤æ˜“éƒ¨\n",
      "éƒ¨é—¨äººæ•°ï¼š19\n",
      "éƒ¨é—¨ä¸»è¦èŒèƒ½ï¼šè´Ÿè´£æŠ•èµ„ç­–ç•¥ç ”å‘åŠå®žç›˜äº¤æ˜“\n",
      "ä¸»è¦è´Ÿè´£äººï¼šé’±æˆï¼›ä¸¥å®\n",
      "\n",
      "éƒ¨é—¨åç§°ï¼šé‡åŒ–ç³»ç»Ÿéƒ¨\n",
      "éƒ¨é—¨äººæ•°ï¼š6\n",
      "éƒ¨é—¨ä¸»è¦èŒèƒ½ï¼šé‡åŒ–å¹³å°å¼€å‘ä¸Žè¿­ä»£ï¼Œä¿éšœæœåŠ¡å™¨ç¨³å®šä¸Žæ•°æ®åº“å®žæ—¶æ›´æ–°\n",
      "ä¸»è¦è´Ÿè´£äººï¼šéƒ‘éš†\n",
      "\n",
      "éƒ¨é—¨åç§°ï¼šé£Žé™©ç®¡ç†éƒ¨\n",
      "éƒ¨é—¨äººæ•°ï¼š2\n",
      "éƒ¨é—¨ä¸»è¦èŒèƒ½ï¼šåˆ¶å®šå¹¶æ‰§è¡Œåˆè§„æ”¿ç­–ä¸Žé£Žé™©ç®¡ç†åˆ¶åº¦ï¼Œåˆè§„å®¡æŸ¥ä¸Žé£Žé™©æŽ§åˆ¶\n",
      "ä¸»è¦è´Ÿè´£äººï¼šå¾çŽ®\n",
      "\n",
      "éƒ¨é—¨åç§°ï¼šå¸‚åœºè¿è¥éƒ¨\n",
      "éƒ¨é—¨äººæ•°ï¼š7\n",
      "éƒ¨é—¨ä¸»è¦èŒèƒ½ï¼šå¸‚åœºæ‹“å±•ã€å®¢æˆ·å…³ç³»ç®¡ç†ã€äº§å“æ—¥å¸¸è¿è¥\n",
      "ä¸»è¦è´Ÿè´£äººï¼šé»„è‰³\n",
      "\n",
      "éƒ¨é—¨åç§°ï¼šç»¼åˆè¡Œæ”¿éƒ¨\n",
      "éƒ¨é—¨äººæ•°ï¼š2\n",
      "éƒ¨é—¨ä¸»è¦èŒèƒ½ï¼šäººåŠ›èµ„æºã€è´¢åŠ¡ç®¡ç†ã€è¡Œæ”¿åŽå‹¤ç›¸å…³å·¥ä½œ\n",
      "ä¸»è¦è´Ÿè´£äººï¼šéŸ©æ–‡æ¨...\n",
      "------------------------------------------------------------\n",
      "âœ… [7.5711] \n",
      "ðŸ“„ ä¸»è¦ç­–ç•¥ç±»åž‹åŠè§„æ¨¡ï¼šé‡åŒ–è‚¡ç¥¨ï¼šæ•°é‡140ä¸ªï¼Œè§„æ¨¡57äº¿ï¼›CTAï¼šæ•°é‡10ä¸ªï¼Œè§„æ¨¡10äº¿ï¼›æ··åˆå¤šç­–ç•¥ï¼šæ•°é‡24ä¸ªï¼Œè§„æ¨¡5äº¿ã€‚...\n",
      "------------------------------------------------------------\n",
      "âœ… [7.2912] \n",
      "ðŸ“„ å…¬å¸ç®€ä»‹ï¼šä¸Šæµ·å®½æŠ•èµ„äº§ç®¡ç†æœ‰é™å…¬å¸æˆç«‹äºŽ2014å¹´12æœˆï¼Œ2015å¹´4æœˆé€šè¿‡ä¸­å›½è¯åˆ¸æŠ•èµ„åŸºé‡‘ä¸šåä¼šå¤‡æ¡ˆï¼Œ2017å¹´4æœˆæˆä¸ºåä¼šä¼šå‘˜å•ä½ï¼Œ2018å¹´4æœˆèŽ·å¾—æŠ•é¡¾èµ„æ ¼ï¼Œ2021å¹´èµ„äº§ç®¡ç†è§„æ¨¡çªç ´100äº¿å…ƒã€‚å…¬å¸ä¸“æ³¨äºŽå›½å†…äºŒçº§å¸‚åœºé‡åŒ–æŠ•èµ„äº¤æ˜“ï¼ŒæŠ•ç ”å›¢é˜Ÿç”±å…·å¤‡å›½å†…å¤–çŸ¥åé«˜æ ¡ç»Ÿè®¡å­¦ã€æ•°å­¦ã€é‡‘èžå·¥ç¨‹ã€è®¡ç®—æœºç§‘å­¦ç­‰ä¸“ä¸šçš„ç¡•å£«æˆ–åšå£«å­¦ä½çš„é¡¶å°–é‡åŒ–ç ”ç©¶äººæ‰æž„æˆï¼Œæ˜¯å›½å†…æœ€æ—©æ¶‰è¶³é‡åŒ–æŠ•èµ„çš„å›¢é˜Ÿä¹‹ä¸€ï¼Œå¯¹äºŽé‡åŒ–åˆ†æžæ–¹æ³•å’Œé‡åŒ–æŠ•èµ„ç­–ç•¥çš„åº”ç”¨å…·å¤‡ä¸°å¯Œçš„å®žç›˜ç»éªŒï¼Œè‡´åŠ›äºŽä¸ºæŠ•èµ„è€…æŒç»­åˆ›é€ ç¨³å¥çš„æŠ•èµ„æ”¶ç›Šã€‚å…¬å¸äº§å“çº¿åŒ…æ‹¬æŒ‡æ•°å¢žå¼ºã€é‡åŒ–é€‰è‚¡ã€æœŸè´§CTAç­‰å¤šç­–ç•¥ä½“ç³»ï¼Œåˆä½œæœºæž„è¦†ç›–å›½å†…ä¸»æµåˆ¸å•†ã€é“¶è¡Œã€ä¿¡æ‰˜ã€æœŸè´§ç­‰å„ç±»é‡‘èžæœºæž„ã€‚...\n",
      "------------------------------------------------------------\n",
      "âœ… [6.9602] \n",
      "ðŸ“„ - **å…¬å¸é£Žé™©ç®¡ç†ç»“æž„**ï¼š1ã€ç®¡ç†å±‚å†³ç­–ï¼šç®¡ç†å±‚å†³ç­–ä¸»è¦è¯„ä¼°å…¬å¸è¿‘æœŸè¿ä½œè¡¨çŽ°ï¼Œå¯¹ä¸‹ä¸€é˜¶æ®µåˆ¶å®šäº¤æ˜“è®¡åˆ’ã€‚åŒæ—¶å¯¹è¿‘æœŸå¸‚åœºé‡å¤§äº‹ä»¶æˆ–æ”¿ç­–åšå‡ºç´§æ€¥åº”å¯¹æŽªæ–½ï¼Œå¯å¼¥è¡¥é‡åŒ–åˆ†æžæ–¹æ³•åœ¨æ­¤ç±»æƒ…å†µä¸­çš„ä¸è¶³ã€‚\n",
      "2ã€äº§å“è®¾è®¡è¦æ±‚ï¼šäº§å“è®¾è®¡ä¸­å‡æœ‰ä¸€å®šçš„é£Žé™©ç®¡ç†æ¡ä¾‹å’Œçº¦æŸï¼Œä¸åŒçš„äº§å“æœ‰å„è‡ªçš„ä»“ä½é™åˆ¶ï¼Œé¢„è­¦çº¿å’Œæ¸…ç›˜çº¿ã€‚åœ¨å®žé™…è¿ä½œä¸­ï¼Œä¸¥æ ¼éµå®ˆäº§å“é£Žé™©ç®¡ç†è¦æ±‚ã€‚\n",
      "3ã€æ¯æ—¥ç›˜åŽè¯„ä¼°ï¼šæ¯æ—¥ç›˜åŽè¯„ä¼°å¯¹è±¡ä¸ºå·²ç»å®žç›˜ä¸Šçº¿çš„äº¤æ˜“ç­–ç•¥ã€‚è¯„ä¼°å„ä¸ªç­–ç•¥å½“å¤©çš„äº¤æ˜“è¡¨çŽ°æ˜¯å¦ä¸Žç­–ç•¥æ¨¡åž‹çš„æ¨¡æ‹Ÿæ•ˆæžœä¸€è‡´ï¼Œä¸”å½“ç­–ç•¥å‡ºçŽ°æˆ–è€…è¶…è¿‡åŽ†å²æœ€å¤§å›žæ’¤æ—¶ï¼Œåº”è¯„ä¼°ç­–ç•¥æ˜¯å¦å¤±æ•ˆè¿˜æ˜¯æžç«¯è¡Œæƒ…å¯¼è‡´ï¼Œå¹¶æ ¹æ®è¯„ä¼°ç»“æžœå†³å®šç­–ç•¥æ˜¯å¦ç»§ç»­è¿è¡Œã€‚\n",
      "4ã€ç­–ç•¥æ¨¡åž‹ï¼šç­–ç•¥æ¨¡åž‹ç§ç±»ç¹å¤šï¼Œæ‹¥æœ‰ä¸...\n",
      "------------------------------------------------------------\n",
      "âœ… [6.9583] \n",
      "ðŸ“„ - **å…¬å¸ä¸šåŠ¡ç®¡ç†æ¨¡å¼**ï¼šå…¬å¸å·²å»ºç«‹é«˜åº¦ä¸“ä¸šåŒ–ã€é‡åŒ–é©±åŠ¨çš„æŠ•èµ„ç®¡ç†ä½“ç³»ï¼ŒæŠ•ç ”å›¢é˜ŸæŒ‰è‚¡ç¥¨ã€æœŸè´§ã€ç³»ç»Ÿå’Œäº¤æ˜“å››å¤§æ¨¡å—åˆ†å·¥åä½œï¼Œå…¨æµç¨‹æµæ°´çº¿å¼ç®¡ç†ï¼Œå½¢æˆâ€œæ•°æ®æ”¶é›†-å› å­æŒ–æŽ˜-ç»„åˆä¼˜åŒ–-ç®—æ³•äº¤æ˜“â€çš„æ ‡å‡†åŒ–é“¾æ¡ã€‚ç­–ç•¥å›¢é˜Ÿæ‹¥æœ‰æˆç†Ÿçš„é‡åŒ–ç­–ç•¥æŠ•èµ„ç»éªŒï¼Œalphaå› å­åº“ç´¯ç§¯é€¾2000ä¸ªä½Žç›¸å…³æ€§å› å­ï¼Œè¦†ç›–åŸºæœ¬é¢ã€é‡ä»·å’Œå¦ç±»å› å­ä¸‰å¤§ç»´åº¦ï¼Œå¹¶å»ºç«‹äº†ä¸°å¯Œçš„ç­–ç•¥åº“åŠæŒç»­è¿­ä»£æœºåˆ¶ã€‚äº§å“è®¾è®¡ä¸Šï¼Œé’ˆå¯¹å®¢æˆ·éœ€æ±‚å¼€å‘å®šåˆ¶åŒ–ç­–ç•¥ç»„åˆï¼ŒåŽ†å²äº§å“è¶…é¢æ”¶ç›Šè¡¨çŽ°ç¨³å®šã€‚é£Žé™©ç®¡ç†ç³»ç»Ÿä¸»è¦åŒ…å«äº‹å‰çš„é£Žé™©è¯†åˆ«å’Œé¢„è­¦ã€äº‹ä¸­çš„ç›‘æŽ§å’Œäº‹åŽçš„é£Žé™©è¯„ä¼°ä¸‰éƒ¨åˆ†ï¼Œåœ¨äº‹å‰ç®¡ç†ä¸­åŸºäºŽé£Žé™©æ¨¡åž‹æ¥å®žçŽ°ï¼Œé£Žé™©æ¨¡åž‹ä»¥ç±»Barraçš„é£Žé™©æ¨¡åž‹ä¸ºä¸»è¦åŸºç¡€ï¼Œå…±åˆ†ä¸ºä¸‰ä¸ªå­æ¨¡å—ï¼Œ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for hit in resp[\"hits\"][\"hits\"]:\n",
    "    score = hit[\"_score\"]\n",
    "    source = hit[\"_source\"]\n",
    "    chunk = source.get(\"chunk\", \"...\")\n",
    "\n",
    "    \n",
    "    print(f\"âœ… [{score:.4f}] \")\n",
    "    print(f\"ðŸ“„ {chunk[:300]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eef175fb-1c29-433e-85d1-3bcdb175b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MAX_LEN = 8192\n",
    "def embed_dense(texts: list[str], max_len: int = EMBED_MAX_LEN) -> list[list[float]]:\n",
    "    \"\"\"è¿”å›ž list[list[float]]ï¼›è‡ªåŠ¨è¿‡æ»¤ç©ºæ–‡æœ¬ï¼›è¾“å‡º 1024 ç»´ python floatã€‚\"\"\"\n",
    "    clean = [t for t in texts if isinstance(t, str) and t.strip()]\n",
    "    if not clean:\n",
    "        return []\n",
    "    out = embedder.encode(\n",
    "        clean,\n",
    "        max_length=max_len,\n",
    "        return_dense=True, return_sparse=False, return_colbert_vecs=False,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    vecs = out[\"dense_vecs\"]\n",
    "    try:\n",
    "        vecs = vecs.tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [[float(x) for x in v] for v in vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532dc033-8761-4a43-8e0d-545a4d1d47a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "491136a1-f1d6-4bd7-8510-1ff87679a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²è¿žæŽ¥ http://127.0.0.1:8000/v1 ï¼Œä½¿ç”¨æ¨¡åž‹ï¼šgpt-3.5-turbo\n",
      "â„¹ï¸ è·³è¿‡ Rerankerï¼ˆè®¾ç½® USE_RERANKER=1 å¯å¯ç”¨ï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, sys, traceback\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ---------- ä¾èµ– ----------\n",
    "# pip install \"openai>=1.5.0\"\n",
    "from openai import OpenAI\n",
    "from transformers.utils.versions import require_version\n",
    "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
    "\n",
    "# ---------- OpenAI å…¼å®¹å®¢æˆ·ç«¯ ----------\n",
    "# å¼ºåˆ¶æœ¬åœ°ç›´è¿žï¼Œé¿å…è¢«ç³»ç»Ÿ/å…¬å¸ä»£ç†åŠ«æŒåˆ°å¤–ç½‘å¯¼è‡´ 502\n",
    "os.environ.setdefault(\"NO_PROXY\", \"127.0.0.1,localhost\")\n",
    "\n",
    "API_PORT = os.getenv(\"API_PORT\", \"8000\")\n",
    "BASE_URL = f\"http://127.0.0.1:{API_PORT}/v1\"\n",
    "API_KEY  = os.getenv(\"API_KEY\", \"0\")      # æœ¬åœ°åŽç«¯é€šå¸¸ä¸æ ¡éªŒ\n",
    "PREF_MODEL_ID = os.getenv(\"MODEL_ID\")     # å¯é€‰ï¼šæ‰‹åŠ¨æŒ‡å®šæ¨¡åž‹ idï¼ˆ/v1/models è¿”å›žçš„ idï¼‰\n",
    "\n",
    "# åˆ›å»ºå®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "def pick_model_id(preferred: str | None = None) -> str:\n",
    "    \"\"\"ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ MODEL_IDï¼›å¦åˆ™ä»Ž /v1/models åˆ—è¡¨é‡Œé€‰ä¸€ä¸ªå¯ç”¨ idã€‚\"\"\"\n",
    "    models = client.models.list()\n",
    "    ids = [m.id for m in models.data] if getattr(models, \"data\", None) else []\n",
    "    if not ids:\n",
    "        raise RuntimeError(\"åŽç«¯æœªè¿”å›žä»»ä½•æ¨¡åž‹ï¼šè¯·ç¡®è®¤ API å·²å¯åŠ¨ä¸” /v1/models å¯è®¿é—®ã€‚\")\n",
    "\n",
    "    if preferred:\n",
    "        if preferred in ids:\n",
    "            return preferred\n",
    "        else:\n",
    "            print(f\"âš ï¸ æŒ‡å®šçš„ MODEL_ID='{preferred}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œå°†æ”¹ç”¨ '{ids[0]}'\")\n",
    "    return ids[0]\n",
    "\n",
    "MODEL_ID = pick_model_id(PREF_MODEL_ID)\n",
    "TIMEOUT_S = float(os.getenv(\"API_TIMEOUT\", \"120\"))\n",
    "\n",
    "print(f\"âœ… å·²è¿žæŽ¥ {BASE_URL} ï¼Œä½¿ç”¨æ¨¡åž‹ï¼š{MODEL_ID}\")\n",
    "\n",
    "# ---------- Prompt æ¨¡æ¿ï¼ˆåŽŸæ ·ä¿ç•™ï¼‰ ----------\n",
    "CLASSIFY_TEMPLATE = \"\"\"\n",
    "# role\n",
    "æ„å›¾åˆ†ç±»å™¨\n",
    "## profile\n",
    "æ ¹æ®åŽ†å²å¯¹è¯å¯¹ç”¨æˆ·è¾“å…¥ query è¿›è¡Œæ„å›¾åˆ†ç±»ã€‚\n",
    "\n",
    "[ç±»åˆ«æ ‡ç­¾ä»¥åŠå®šä¹‰]\n",
    "1.é—²èŠ - ä¸æ¶‰åŠä»»åŠ¡ã€ä¿¡æ¯æˆ–å’¨è¯¢çš„éšæ„èŠå¤©å†…å®¹\n",
    "2.æ“¦è¾¹ - æ¶‰åŠæ•æ„Ÿè¯ã€æ€§æš—ç¤ºã€ä¸é€‚å®œå…¬å¼€åœºåˆçš„å†…å®¹\n",
    "3.æ”¿æ²»äººç‰©æ‰¹åˆ¤ - åŒ…å«å¯¹æ”¿åºœã€å®˜å‘˜æˆ–æ”¿ç­–çš„è´Ÿé¢è¯„è®ºï¼ˆä¸åŒ…æ‹¬æ­£å¸¸çš„è´¢ç»æ–°é—»å’¨è¯¢ï¼‰\n",
    "4.é‡‘èžç›¸å…³å’¨è¯¢ - åŒ…æ‹¬ä¸ªè‚¡ã€è¡Œä¸šã€åŸºé‡‘ã€æŒ‡æ ‡ã€è´¢åŠ¡ã€ä¼°å€¼ã€å¸‚åœºåŠ¨æ€ã€ç»æµŽæ•°æ®ç­‰é‡‘èžæŠ•èµ„ç±»é—®é¢˜\n",
    "\n",
    "ã€å‚è€ƒä¾‹å­ã€‘\n",
    "query: pussy\n",
    "label: æ“¦è¾¹\n",
    "\n",
    "query: ç‰¹æœ—æ™®é€€ä¼‘äº†å—\n",
    "label: æ”¿æ²»äººç‰©æ‰¹åˆ¤\n",
    "\n",
    "query: èŒ…å°è‚¡ç¥¨æ³¢åŠ¨å¤§å—\n",
    "label: é‡‘èžç›¸å…³å’¨è¯¢\n",
    "\n",
    "query: è´µå·žèŒ…å°ä»Šå¹´è´¢æŠ¥å‘å¸ƒäº†å—\n",
    "label: é‡‘èžç›¸å…³å’¨è¯¢\n",
    "\n",
    "query: ä½ å–œæ¬¢ä»€ä¹ˆé¢œè‰²ï¼Ÿ\n",
    "label: é—²èŠ\n",
    "\n",
    "[åŽ†å²å¯¹è¯]ï¼ˆå¦‚æžœæ— åŽ†å²è¯·å¿½ç•¥ï¼‰\n",
    "{history_dialogue}\n",
    "\n",
    "ã€ç”¨æˆ·queryã€‘\n",
    "{query}\n",
    "\n",
    "è¯·ç›´æŽ¥è¾“å‡º query çš„æ„å›¾ç»“æžœã€‚\n",
    "\"\"\".strip()\n",
    "\n",
    "QUERY_REWRITE_TEMPLATE = \"\"\"\n",
    "æ ¹æ®ä¸Šä¸‹æ–‡åŽ†å²å¯¹è¯å¯¹ query æ”¹å†™ï¼Œè¿›è¡Œè¡¥å…¨/æŒ‡ä»£/å®Œæ•´çš„æ”¹å†™ï¼Œéžå£è¯­åŒ–ï¼Œå…³é”®è¯æå–ï¼Œä¾¿äºŽåšæ£€ç´¢\n",
    "\n",
    "ã€åŽ†å²å¯¹è¯ã€‘æœ€è¿‘5è½®\n",
    "{history_dialogue}\n",
    "\n",
    "ã€queryã€‘\n",
    "{query}\n",
    "\n",
    "ã€å‚è€ƒä¾‹å­ã€‘\n",
    "- åŽ†å²å¯¹è¯\n",
    "human: æˆ‘æƒ³é—®ä¸€ä¸‹aæ¬¾è¿™ä¸ªè½¦æ˜¯å‰é©±è¿˜æ˜¯åŽé©±\n",
    "bot: åŽé©±\n",
    "human: é‚£bå‘¢\n",
    "- è¾“å‡ºç»“æžœ\n",
    "æ”¹å†™åŽçš„query: é‚£bæ¬¾çš„è½¦å­æ˜¯å‰é©±è¿˜æ˜¯åŽé©±?\n",
    "æå–çš„å…³é”®è¯:bæ¬¾, å‰é©±, åŽé©±, æ±½è½¦\n",
    "\n",
    "è¯·ç›´æŽ¥è¾“å‡ºæ”¹å†™ç»“æžœã€‚\n",
    "\n",
    "æ”¹å†™åŽçš„query:\n",
    "æå–çš„å…³é”®è¯:\n",
    "\"\"\".lstrip()\n",
    "\n",
    "ANSWER_PROMPT_TMPL = \"\"\"\n",
    "# è§’è‰²ï¼šé‡‘èžå’¨è¯¢åŠ©æ‰‹\n",
    "- åªå›žç­”ä¸Žé‡‘èžæŠ•èµ„/å®è§‚ç»æµŽ/è´¢æŠ¥è§£è¯»ç›¸å…³çš„é—®é¢˜  \n",
    "- å›žç­”å¿…é¡»åŸºäºŽã€èµ„æ–™å—ã€‘ç»™å‡ºçš„äº‹å®žï¼Œè‹¥æ— ä¿¡æ¯è¯·å›žç­” â€œæˆ‘ä¸ç¡®å®šâ€\n",
    "\n",
    "## è§„åˆ™\n",
    "1. å†…å®¹å¥åº·å‹å¥½ï¼Œè¯­è¨€ç®€æ´æ˜Žäº†ï¼Œæœ‰é€»è¾‘\n",
    "2. å…ˆç»™ç»“è®ºï¼Œå†åˆ—å¼•ç”¨ç¼–å·ï¼›ç»“è®ºä¹‹å¤–ä¸è¦é€éœ²æŽ¨ç†è¿‡ç¨‹\n",
    "3. æ— æ³•å›žç­”æ—¶ç›´æŽ¥è¯´ â€œæˆ‘ä¸ç¡®å®šâ€ï¼Œä¸è¦ç¼–é€ \n",
    "4. å›žç­”ä¸­çš„ä»»ä½•æ•°å­—/äº‹å®žéƒ½è¦èƒ½åœ¨å¼•ç”¨é‡Œæ‰¾åˆ°\n",
    "\n",
    "## è¾“å…¥\n",
    "### èµ„æ–™å—ï¼ˆå·²æŒ‰ç›¸å…³åº¦æŽ’åºï¼‰\n",
    "{context_blocks}\n",
    "\n",
    "### ç”¨æˆ·é—®é¢˜\n",
    "{query}\n",
    "\n",
    "## è¾“å‡ºæ ¼å¼\n",
    "ç­”æ¡ˆï¼š<ä¸€æ®µåˆ°ä¸¤æ®µç›´æŽ¥ç»™å‡ºç»“è®º>  \n",
    "å¼•ç”¨ï¼š<ç”¨ â‘ â‘¡â‘¢â€¦ æ ‡æ³¨ç”¨åˆ°çš„èµ„æ–™ç¼–å·ï¼Œå¯å¤šä¸ªï¼Œä¾‹å¦‚â€œâ‘ â‘¢â€>\n",
    "\"\"\".strip()\n",
    "\n",
    "# ---------- OpenAI ç‰ˆ llm_invoke ----------\n",
    "def llm_invoke(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = \"\",\n",
    "    *,\n",
    "    temperature: float = 0.2,\n",
    "    max_new_tokens: int = 512,\n",
    "    top_p: float = 0.7,\n",
    ") -> str:\n",
    "    # OpenAI å‚æ•°å« max_tokens\n",
    "    EPS = 1e-3\n",
    "    t = max(float(temperature), EPS)\n",
    "    p = max(float(top_p), EPS)\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=messages,\n",
    "            temperature=t,\n",
    "            top_p=p,\n",
    "            max_tokens=int(max_new_tokens),\n",
    "            timeout=TIMEOUT_S,\n",
    "        )\n",
    "        return (resp.choices[0].message.content or \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(\"âŒ è°ƒç”¨èŠå¤©æŽ¥å£å¤±è´¥ï¼š\", e)\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "# ---------- å·¥å…·å‡½æ•°ï¼ˆåŽŸæ ·ä¿ç•™/å¾®è°ƒï¼‰ ----------\n",
    "def build_history(dialogue: list, max_turns: int = 5) -> str:\n",
    "    pairs = []\n",
    "    for m in dialogue[-max_turns*2:]:\n",
    "        role = \"human\" if m[\"role\"] == \"user\" else \"bot\"\n",
    "        pairs.append(f\"{role}: {m['content']}\")\n",
    "    return \"\\n\".join(pairs) or \"ï¼ˆæ— åŽ†å²ï¼‰\"\n",
    "\n",
    "def classify_query(history_dialogue: str, query: str) -> str:\n",
    "    prompt = CLASSIFY_TEMPLATE.format(history_dialogue=history_dialogue, query=query)\n",
    "    sys = \"åªè¾“å‡ºä¸€ä¸ªä¸­æ–‡æ ‡ç­¾ï¼šé—²èŠ / æ“¦è¾¹ / æ”¿æ²»äººç‰©æ‰¹åˆ¤ / é‡‘èžç›¸å…³å’¨è¯¢ã€‚ä¸è¦é™„åŠ ä»»ä½•è§£é‡Šæˆ–å‰åŽç¼€ã€‚\"\n",
    "    out = llm_invoke(prompt, system_prompt=sys, temperature=0.01, max_new_tokens=64, top_p=0.1)\n",
    "    out = re.sub(r'^\\s*(label[:ï¼š]?)\\s*', '', out or '').strip()\n",
    "    return out\n",
    "\n",
    "def _parse_rewrite_output(text: str, fallback_query: str):\n",
    "    text = text or \"\"\n",
    "    m_q  = re.search(r\"æ”¹å†™åŽçš„?query\\s*[:ï¼š]\\s*(.+)\", text)\n",
    "    m_kw = re.search(r\"æå–çš„?å…³é”®è¯\\s*[:ï¼š]\\s*(.+)\", text)\n",
    "    rewrite  = (m_q.group(1).strip() if m_q else fallback_query).strip()\n",
    "    keywords = [k.strip() for k in (m_kw.group(1).split(\",\"))] if m_kw else []\n",
    "    rewrite  = rewrite.strip(\"`\").strip()\n",
    "    keywords = [k for k in keywords if k]\n",
    "    return rewrite, keywords\n",
    "\n",
    "def rewrite_query(dialogue: list, query: str):\n",
    "    hist = build_history(dialogue)\n",
    "    prompt = QUERY_REWRITE_TEMPLATE.format(history_dialogue=hist, query=query)\n",
    "    sys = \"ä¸¥æ ¼æŒ‰æ¨¡æ¿é”®åè¾“å‡ºï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚\"\n",
    "    content = llm_invoke(prompt, system_prompt=sys, temperature=0.3, max_new_tokens=256)\n",
    "    rewrite, keywords = _parse_rewrite_output(content, fallback_query=query)\n",
    "    return rewrite, keywords, content\n",
    "\n",
    "def build_context(passages: List[str], top_k: int = 5) -> str:\n",
    "    items = [p for p in passages[:top_k] if p and p.strip()]\n",
    "    circled_nums = \"â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\"\n",
    "    return \"\\n\\n\".join(f\"{circled_nums[i]} {p}\" for i, p in enumerate(items))\n",
    "\n",
    "def generate_answer(passages: List[str], query: str, temperature: float = 0.2, max_new_tokens: int = 512) -> str:\n",
    "    context = build_context(passages)\n",
    "    prompt  = ANSWER_PROMPT_TMPL.format(context_blocks=context, query=query)\n",
    "    sys = \"ä½ æ˜¯ä¸“ä¸šçš„é‡‘èžå’¨è¯¢åŠ©æ‰‹ã€‚\"\n",
    "    return llm_invoke(\n",
    "        prompt,\n",
    "        system_prompt=sys,\n",
    "        temperature=float(temperature),\n",
    "        max_new_tokens=int(max_new_tokens),\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- ï¼ˆå¯é€‰ï¼‰Rerankerï¼ˆä¿æŒä½ çš„é™çº§é€»è¾‘ï¼‰ ----------\n",
    "# try:\n",
    "#    from FlagEmbedding import FlagReranker\n",
    "#    import torch\n",
    "#    _device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#    reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=_device==\"cuda\")\n",
    "# except Exception as e:\n",
    "#    reranker = None\n",
    "#    print(\"âš ï¸ æœªå¯ç”¨ Rerankerï¼ˆFlagEmbedding æœªå®‰è£…æˆ–æ¨¡åž‹ä¸å¯ç”¨ï¼‰ï¼š\", e)\n",
    "\n",
    "def rerank_documents(query: str, initial_docs: List[str], top_n: int = 5) -> List[Dict[str, Any]]:\n",
    "    if not initial_docs:\n",
    "        return []\n",
    "    if reranker is None:\n",
    "       return [{\"score\": 0.0, \"content\": c} for c in initial_docs[:top_n]]\n",
    "    sentence_pairs = [[query, passage] for passage in initial_docs]\n",
    "    scores = reranker.compute_score(sentence_pairs)\n",
    "#   scored = [{\"score\": float(s), \"content\": c} for s, c in zip(scores, initial_docs)]\n",
    "    return sorted(scored, key=lambda x: x[\"score\"], reverse=True)[:top_n]\n",
    "\n",
    "# ===== åœ¨ã€ŒRerankerã€è¿™ä¸€æ®µæ›¿æ¢æˆä¸‹é¢è¿™æ®µ =====\n",
    "USE_RERANKER = os.getenv(\"USE_RERANKER\", \"0\") == \"1\"        # é»˜è®¤å…³é—­ï¼Œæƒ³å¼€æ—¶è®¾ç½®çŽ¯å¢ƒå˜é‡ USE_RERANKER=1\n",
    "RERANKER_ID = os.getenv(\"RERANKER_ID\", \"BAAI/bge-reranker-v2-m3\")\n",
    "RERANKER_LOCAL_ONLY = os.getenv(\"RERANKER_LOCAL_ONLY\", \"1\") == \"1\"  # é»˜è®¤ä»…ç¦»çº¿åŠ è½½ï¼Œé¿å…ç½‘ç»œè¶…æ—¶\n",
    "\n",
    "reranker = None\n",
    "if USE_RERANKER:\n",
    "    try:\n",
    "        import torch\n",
    "        from FlagEmbedding import FlagReranker\n",
    "        if RERANKER_LOCAL_ONLY:\n",
    "            os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")  # æ²¡æœ‰æœ¬åœ°ç¼“å­˜ä¼šç«‹åˆ»æŠ¥é”™è€Œä¸æ˜¯åå¤é‡è¯•\n",
    "        reranker = FlagReranker(\n",
    "            RERANKER_ID,\n",
    "            use_fp16=torch.cuda.is_available(),\n",
    "            local_files_only=RERANKER_LOCAL_ONLY,\n",
    "        )\n",
    "        print(f\"âœ… Reranker å·²å¯ç”¨ï¼š{RERANKER_ID}\")\n",
    "    except Exception as e:\n",
    "        reranker = None\n",
    "        print(\"âš ï¸ æœªå¯ç”¨ Rerankerï¼ˆé™çº§è¿è¡Œï¼‰ï¼š\", e)\n",
    "else:\n",
    "    print(\"â„¹ï¸ è·³è¿‡ Rerankerï¼ˆè®¾ç½® USE_RERANKER=1 å¯å¯ç”¨ï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "# ---------- Elasticsearch ç¤ºä¾‹ï¼ˆä¸Žä½ åŽŸæ¥çš„å®žçŽ°ä¿æŒä¸€è‡´ï¼Œå ä½ï¼‰ ----------\n",
    "# ---------- Elasticsearchï¼šæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ + section åŠ æƒï¼‰ ----------\n",
    "def _has_section_fields(index_name: str = \"dd_report_data\") -> bool:\n",
    "    try:\n",
    "        props = es.indices.get_mapping(index=index_name)[index_name][\"mappings\"][\"properties\"]\n",
    "        return (\"section\" in props) or (\"section_text\" in props)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _hybrid_query_dual(\n",
    "    q1_text: str, q1_vec: list[float],\n",
    "    q2_text: str | None, q2_vec: list[float] | None,\n",
    "    has_section: bool,\n",
    "    dense_weight: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    q1 = æ”¹å†™åŽçš„ queryï¼ˆä¸»æŸ¥è¯¢ï¼Œæƒé‡æ›´é«˜ï¼‰\n",
    "    q2 = åŽŸå§‹ç”¨æˆ·è¾“å…¥ï¼ˆå‰¯æŸ¥è¯¢ï¼Œæƒé‡ç¨ä½Žï¼›ä¸ºç©ºåˆ™ä¸ä½¿ç”¨ï¼‰\n",
    "    \"\"\"\n",
    "    # çŸ­æŸ¥è¯¢ï¼ˆ<=4æ±‰å­—ï¼‰åŠ å¤§ç¨€ç–ã€é™ä½Žå‘é‡\n",
    "    def _short(q: str) -> bool:\n",
    "        return len(q) <= 4 and all('\\u4e00' <= ch <= '\\u9fff' for ch in q)\n",
    "\n",
    "    is_short_q1 = _short(q1_text)\n",
    "    dense_w_q1  = 0.5 if is_short_q1 else dense_weight\n",
    "    sparse_boost_q1 = 4.0 if is_short_q1 else 1.5\n",
    "\n",
    "    is_short_q2 = _short(q2_text) if q2_text else False\n",
    "    dense_w_q2  = 0.4 if is_short_q2 else 0.7 * dense_weight\n",
    "    sparse_boost_q2 = 3.0 if is_short_q2 else 1.2\n",
    "\n",
    "    fields = [\"chunk^1\"]\n",
    "    if has_section:\n",
    "        fields.append(\"section_text^5\")\n",
    "\n",
    "    should_clauses = [\n",
    "        # q1ï¼ˆæ”¹å†™ï¼‰â€”â€”ä¸»åŠ›\n",
    "        {\"multi_match\": {\"query\": q1_text, \"fields\": fields, \"type\": \"most_fields\", \"boost\": 2.0}},\n",
    "        {\"match_phrase\": {\"chunk\": {\"query\": q1_text, \"boost\": 3.0}}},\n",
    "    ]\n",
    "    if has_section:\n",
    "        should_clauses.append({\"term\": {\"section\": {\"value\": q1_text, \"boost\": sparse_boost_q1}}})\n",
    "\n",
    "    # q2ï¼ˆåŽŸå§‹ï¼‰â€”â€”è¾…åŠ©\n",
    "    if q2_text and q2_text.strip() and q2_text.strip() != q1_text.strip():\n",
    "        should_clauses.extend([\n",
    "            {\"multi_match\": {\"query\": q2_text, \"fields\": fields, \"type\": \"most_fields\", \"boost\": 1.2}},\n",
    "            {\"match_phrase\": {\"chunk\": {\"query\": q2_text, \"boost\": 2.0}}},\n",
    "        ])\n",
    "        if has_section:\n",
    "            should_clauses.append({\"term\": {\"section\": {\"value\": q2_text, \"boost\": sparse_boost_q2}}})\n",
    "\n",
    "    base_bool = {\"should\": should_clauses, \"minimum_should_match\": 1}\n",
    "\n",
    "    functions = [{\n",
    "        \"script_score\": {\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.qv1, doc['chunk_vector']) + 1.0\",\n",
    "                \"params\": {\"qv1\": q1_vec}\n",
    "            }\n",
    "        },\n",
    "        \"weight\": dense_w_q1\n",
    "    }]\n",
    "    if q2_vec is not None:\n",
    "        functions.append({\n",
    "            \"script_score\": {\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.qv2, doc['chunk_vector']) + 1.0\",\n",
    "                    \"params\": {\"qv2\": q2_vec}\n",
    "                }\n",
    "            },\n",
    "            \"weight\": dense_w_q2\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"function_score\": {\n",
    "            \"query\": {\"bool\": base_bool},\n",
    "            \"boost_mode\": \"sum\",\n",
    "            \"score_mode\": \"sum\",\n",
    "            \"functions\": functions\n",
    "        }\n",
    "    }\n",
    "\n",
    "def es_hybrid_search(\n",
    "    rewrite_text: str,\n",
    "    raw_text: str | None = None,\n",
    "    index_name: str = \"dd_report_data\",\n",
    "    size: int = 20,\n",
    "):\n",
    "    # ç”Ÿæˆä¸¤ä¸ªå‘é‡ï¼ˆç›¸åŒåˆ™åªç®—ä¸€æ¬¡ï¼‰\n",
    "    vec1 = embed([rewrite_text])\n",
    "    vec2 = None\n",
    "    if raw_text and raw_text.strip() and raw_text.strip() != rewrite_text.strip():\n",
    "        vec2 = embed_dense([raw_text])[0]\n",
    "\n",
    "    has_section = _has_section_fields(index_name)\n",
    "    query_body = _hybrid_query_dual(\n",
    "        q1_text=rewrite_text, q1_vec=vec1,\n",
    "        q2_text=raw_text, q2_vec=vec2,\n",
    "        has_section=has_section,\n",
    "        dense_weight=1.0\n",
    "    )\n",
    "\n",
    "    return es.search(                                      # noqa: F821\n",
    "        index=index_name,\n",
    "        size=size,\n",
    "        query=query_body,\n",
    "        _source=[\"doc_id\",\"block_id\",\"part_id\",\"section\",\"section_text\",\"chunk\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# ---------- äº¤äº’ä¸»å¾ªçŽ¯ï¼ˆæ— è”ç½‘æœç´¢ï¼‰ ----------\n",
    "def main():\n",
    "    print(\"âœ… å°±ç»ªï¼šè¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ exit é€€å‡ºã€‚\")\n",
    "    conversation: List[Dict[str, str]] = []\n",
    "    turn = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\nBye!\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        turn += 1\n",
    "        print(\"\\n\" + \"â”€\" * 80)\n",
    "        print(f\"ðŸ—£ï¸ ç¬¬ {turn} è½® - ç”¨æˆ·é—®é¢˜ï¼š{user_input}\", flush=True)\n",
    "        print(\"â”€\" * 80 + \"\\n\")\n",
    "\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # 1) åˆ†ç±»\n",
    "        history_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in conversation[-6:-1]])\n",
    "        label = classify_query(history_text, user_input)\n",
    "        print(\"ðŸ“Œ åˆ†ç±»ç»“æžœ:\", label)\n",
    "\n",
    "        if \"é‡‘èž\" not in label:\n",
    "            print(\"âŒ éžé‡‘èžé—®é¢˜ï¼Œä¸äºˆå¤„ç†\")\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": \"å¯¹ä¸èµ·ï¼Œæˆ‘åªèƒ½å¤„ç†é‡‘èžç›¸å…³é—®é¢˜ã€‚\"})\n",
    "            continue\n",
    "\n",
    "        # 2) æ”¹å†™\n",
    "        rewrite, keywords, raw = rewrite_query(conversation, user_input)\n",
    "        print(\"âœï¸ æ”¹å†™åŽçš„ Query:\", rewrite)\n",
    "        print(\"ðŸ”‘ æå–çš„å…³é”®è¯:\", keywords)\n",
    "\n",
    "        # 3) ä»Ž ES å¬å›ž\n",
    "        try:\n",
    "            resp = es_hybrid_search(\n",
    "                        rewrite_text=rewrite,\n",
    "                        raw_text=user_input,                  # â† åŒæ—¶ç”¨åŽŸå§‹è¾“å…¥\n",
    "                        index_name=\"dd_report_data\",\n",
    "                        size=20)\n",
    "            kb_ctx = [h[\"_source\"].get(\"chunk\",\"\") for h in resp[\"hits\"][\"hits\"]]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ ES æŸ¥è¯¢å¤±è´¥ï¼ŒæŒ‰æ— æœ¬åœ°çŸ¥è¯†å¤„ç†ï¼š\", e)\n",
    "            kb_ctx = []\n",
    "\n",
    "        if kb_ctx:\n",
    "            print(f\"ðŸ“š æ¥è‡ª ES çš„å€™é€‰æ®µè½ï¼š{len(kb_ctx)} æ¡\")\n",
    "        else:\n",
    "            print(\"ðŸ“š ES æš‚æ— å‘½ä¸­\")\n",
    "\n",
    "        # 4) é‡æŽ’åºï¼ˆå¯é€‰ï¼‰\n",
    "        merged   = kb_ctx\n",
    "        top_docs = rerank_documents(rewrite, merged, top_n=10)\n",
    "        passages_for_answer = [d[\"content\"] for d in top_docs] if top_docs else merged\n",
    "\n",
    "        # 5) ç”Ÿæˆç­”æ¡ˆï¼ˆåªç”¨æœ¬åœ°çŸ¥è¯†ï¼‰\n",
    "        answer = generate_answer(passages_for_answer, query=rewrite)\n",
    "        print(\"\\nðŸ§  æ¨¡åž‹å›žç­”ï¼š\\n\", answer)\n",
    "\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "989bcbcb-9897-48b8-a525-0a599dc7cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å°±ç»ªï¼šè¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ exit é€€å‡ºã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  å…¬å¸ç®€ä»‹\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ—£ï¸ ç¬¬ 1 è½® - ç”¨æˆ·é—®é¢˜ï¼šå…¬å¸ç®€ä»‹\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸ“Œ åˆ†ç±»ç»“æžœ: é‡‘èžç›¸å…³å’¨è¯¢\n",
      "âœï¸ æ”¹å†™åŽçš„ Query: å…¬å¸ç®€ä»‹\n",
      "ðŸ”‘ æå–çš„å…³é”®è¯: []\n",
      "ðŸ“š æ¥è‡ª ES çš„å€™é€‰æ®µè½ï¼š20 æ¡\n",
      "\n",
      "ðŸ§  æ¨¡åž‹å›žç­”ï¼š\n",
      " ä¸Šæµ·å®½æŠ•èµ„äº§ç®¡ç†æœ‰é™å…¬å¸æˆç«‹äºŽ2014å¹´12æœˆï¼Œ2015å¹´4æœˆé€šè¿‡ä¸­å›½è¯åˆ¸æŠ•èµ„åŸºé‡‘ä¸šåä¼šå¤‡æ¡ˆï¼Œ2017å¹´4æœˆæˆä¸ºåä¼šä¼šå‘˜å•ä½ï¼Œ2018å¹´4æœˆèŽ·å¾—æŠ•é¡¾èµ„æ ¼ï¼Œ2021å¹´èµ„äº§ç®¡ç†è§„æ¨¡çªç ´100äº¿å…ƒã€‚å…¬å¸ä¸“æ³¨äºŽå›½å†…äºŒçº§å¸‚åœºé‡åŒ–æŠ•èµ„äº¤æ˜“ï¼ŒæŠ•ç ”å›¢é˜Ÿç”±å…·å¤‡å›½å†…å¤–çŸ¥åé«˜æ ¡ç»Ÿè®¡å­¦ã€æ•°å­¦ã€é‡‘èžå·¥ç¨‹ã€è®¡ç®—æœºç§‘å­¦ç­‰ä¸“ä¸šçš„ç¡•å£«æˆ–åšå£«å­¦ä½çš„é¡¶å°–é‡åŒ–ç ”ç©¶äººæ‰æž„æˆï¼Œæ˜¯å›½å†…æœ€æ—©æ¶‰è¶³é‡åŒ–æŠ•èµ„çš„å›¢é˜Ÿä¹‹ä¸€ï¼Œå¯¹äºŽé‡åŒ–åˆ†æžæ–¹æ³•å’Œé‡åŒ–æŠ•èµ„ç­–ç•¥çš„åº”ç”¨å…·å¤‡ä¸°å¯Œçš„å®žç›˜ç»éªŒï¼Œè‡´åŠ›äºŽä¸ºæŠ•èµ„è€…æŒç»­åˆ›é€ ç¨³å¥çš„æŠ•èµ„æ”¶ç›Šã€‚å…¬å¸äº§å“çº¿åŒ…æ‹¬æŒ‡æ•°å¢žå¼ºã€é‡åŒ–é€‰è‚¡ã€æœŸè´§CTAç­‰å¤šç­–ç•¥ä½“ç³»ï¼Œåˆä½œæœºæž„è¦†ç›–å›½å†…ä¸»æµåˆ¸å•†ã€é“¶è¡Œã€ä¿¡æ‰˜ã€æœŸè´§ç­‰å„ç±»é‡‘èžæœºæž„â‘ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2677bebc-d9d6-41aa-8f9c-e559bc37a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sxmxs\\AppData\\Local\\Temp\\ipykernel_9012\\1205298531.py:119: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=420, show_copy_button=True, label=\"å¯¹è¯\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: The vector functions of the form function(query, doc['field']) are deprecated, and the form function(query, 'field') should be used instead. For example, cosineSimilarity(query, doc['field']) is replaced by cosineSimilarity(query, 'field').\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: The vector functions of the form function(query, doc['field']) are deprecated, and the form function(query, 'field') should be used instead. For example, cosineSimilarity(query, doc['field']) is replaced by cosineSimilarity(query, 'field').\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "D:\\anaconda\\envs\\es\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: The vector functions of the form function(query, doc['field']) are deprecated, and the form function(query, 'field') should be used instead. For example, cosineSimilarity(query, doc['field']) is replaced by cosineSimilarity(query, 'field').\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "# ====================== Gradio UI ======================\n",
    "import gradio as gr\n",
    "import datetime as _dt\n",
    "from typing import Dict, List  # â† è¡¥ä¸Š\n",
    "\n",
    "DEFAULT_INDEX = \"dd_report_data\"\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "# 1) Chatbot ç”¨ messages æ¨¡å¼\n",
    "chatbot = gr.Chatbot(type=\"messages\", height=420, show_copy_button=True, label=\"å¯¹è¯\")\n",
    "\n",
    "# 2) åŽ†å²è½¬æ¢ï¼šè‹¥å·²ç»æ˜¯ messagesï¼Œç›´æŽ¥è¿”å›žï¼›å¦åˆ™ä»Ž (u, a) è½¬\n",
    "def _history_to_dialogue(chat_history: list) -> List[Dict[str, str]]:\n",
    "    if chat_history and isinstance(chat_history[0], dict):  # messages æ¨¡å¼\n",
    "        return chat_history\n",
    "    conv = []\n",
    "    for u, a in chat_history:\n",
    "        if u: conv.append({\"role\": \"user\", \"content\": u})\n",
    "        if a: conv.append({\"role\": \"assistant\", \"content\": a})\n",
    "    return conv\n",
    "\n",
    "# 3) è¿½åŠ å¯¹è¯ï¼šmessages éœ€è¦ä¸¤æ¡ message\n",
    "def _append(chat_history, user, assistant):\n",
    "    if chat_history and isinstance(chat_history[0], dict):\n",
    "        return chat_history + [\n",
    "            {\"role\":\"user\",\"content\":user},\n",
    "            {\"role\":\"assistant\",\"content\":assistant},\n",
    "        ]\n",
    "    return chat_history + [(user, assistant)]\n",
    "\n",
    "\n",
    "def _format_hits_md(resp, topn: int = 8, *, show_full: bool = False, snippet_chars: int = 4096) -> str:\n",
    "    \"\"\"\n",
    "    show_full=True  -> æ˜¾ç¤ºå…¨æ–‡\n",
    "    show_full=False -> åªæ˜¾ç¤ºå‰ snippet_chars ä¸ªå­—ç¬¦ï¼ˆé»˜è®¤ 800ï¼‰\n",
    "    \"\"\"\n",
    "    if not resp or \"hits\" not in resp or not resp[\"hits\"][\"hits\"]:\n",
    "        return \"ï¼ˆæ— å‘½ä¸­ï¼‰\"\n",
    "    lines = []\n",
    "    for i, h in enumerate(resp[\"hits\"][\"hits\"][:topn], 1):\n",
    "        sc = h.get(\"_score\", 0.0)\n",
    "        src = h.get(\"_source\", {})\n",
    "        sec = src.get(\"section\") or src.get(\"section_text\") or \"â€”\"\n",
    "        txt = (src.get(\"chunk\") or \"\").strip().replace(\"\\r\\n\", \"\\n\")\n",
    "        if not show_full and len(txt) > snippet_chars:\n",
    "            body = txt[:snippet_chars] + \"â€¦\"\n",
    "        else:\n",
    "            body = txt\n",
    "        # ç”¨ <details> æŠ˜å ï¼ˆGradio Markdown æ”¯æŒåŸºç¡€ HTMLï¼Œå…¼å®¹é•¿æ–‡ï¼‰\n",
    "        html = (\n",
    "            f\"<details open><summary><b>{i:02d}. [{sc:.3f}] {sec}</b></summary>\"\n",
    "            f\"<div style='white-space:pre-wrap'>{body}</div></details>\"\n",
    "        )\n",
    "        lines.append(html)\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def respond(message, chat_history, temperature, n_ctx, index_name,\n",
    "            show_full_hits, snippet_chars, answer_len):\n",
    "    # 1) åŽ†å²è½¬æ¢ + åˆ†ç±»\n",
    "    conversation = _history_to_dialogue(chat_history)\n",
    "    history_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in conversation[-6:]])\n",
    "    label = classify_query(history_text, message) or \"ï¼ˆæœªè¯†åˆ«ï¼‰\"\n",
    "\n",
    "    if \"é‡‘èž\" not in label:\n",
    "        assistant = \"å¯¹ä¸èµ·ï¼Œæˆ‘åªèƒ½å¤„ç†é‡‘èžç›¸å…³é—®é¢˜ã€‚\"\n",
    "        chat_history = _append(chat_history, message, assistant)\n",
    "        debug_md = f\"**åˆ†ç±»**ï¼š{label}  \\n**æ—¶é—´**ï¼š{_dt.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "        return chat_history, \"ï¼ˆæ— æ£€ç´¢ï¼‰\", debug_md\n",
    "\n",
    "    # 2) æ”¹å†™\n",
    "    rewrite, keywords, _raw = rewrite_query(conversation + [{\"role\": \"user\", \"content\": message}], message)\n",
    "\n",
    "    # 3) æ£€ç´¢\n",
    "    try:\n",
    "        resp = es_hybrid_search(\n",
    "            rewrite_text=rewrite,\n",
    "            raw_text=message,\n",
    "            index_name=index_name or DEFAULT_INDEX,\n",
    "            size=int(n_ctx),\n",
    "        )\n",
    "        kb_ctx = [h[\"_source\"].get(\"chunk\", \"\") for h in resp[\"hits\"][\"hits\"]]\n",
    "        hits_md = _format_hits_md(resp, topn=min(8, int(n_ctx)),\n",
    "                                  show_full=bool(show_full_hits), snippet_chars=int(snippet_chars))\n",
    "    except Exception as e:\n",
    "        kb_ctx, hits_md = [], f\"æ£€ç´¢å¤±è´¥ï¼š`{repr(e)}`\"\n",
    "\n",
    "    # 4) ï¼ˆå¯é€‰ï¼‰é‡æŽ’åº\n",
    "    merged = kb_ctx\n",
    "    top_docs = rerank_documents(rewrite, merged, top_n=min(10, len(merged))) if merged else []\n",
    "    passages_for_answer = [d[\"content\"] for d in top_docs] if top_docs else merged\n",
    "\n",
    "    # 5) ç”Ÿæˆï¼ˆæŠŠæ¸©åº¦ + æœ€å¤§é•¿åº¦ä¼ è¿›åŽ»ï¼‰\n",
    "    temp = float(temperature) if temperature is not None else 0.2\n",
    "    answer = generate_answer(passages_for_answer[:5], query=rewrite,\n",
    "                             temperature=temp, max_new_tokens=int(answer_len))\n",
    "    assistant = answer or \"æˆ‘ä¸ç¡®å®šã€‚\"\n",
    "    chat_history = _append(chat_history, message, assistant)\n",
    "\n",
    "    # 6) è°ƒè¯•ä¿¡æ¯\n",
    "    debug_md = (\n",
    "        f\"**åˆ†ç±»**ï¼š{label}\\n\\n\"\n",
    "        f\"**åŽŸå§‹é—®é¢˜**ï¼š{message}\\n\\n\"\n",
    "        f\"**æ”¹å†™åŽ**ï¼š{rewrite}\\n\\n\"\n",
    "        f\"**å…³é”®è¯**ï¼š{', '.join(keywords) if keywords else 'â€”'}\\n\\n\"\n",
    "        f\"**ç´¢å¼•**ï¼š{index_name or DEFAULT_INDEX}\\n\\n\"\n",
    "        f\"**æ¸©åº¦**ï¼š{temp} | **ç­”æ¡ˆä¸Šé™**ï¼š{int(answer_len)} tokens\\n\\n\"\n",
    "        f\"**æ—¶é—´**ï¼š{_dt.datetime.now():%Y-%m-%d %H:%M:%S}\"\n",
    "    )\n",
    "    return chat_history, hits_md, debug_md\n",
    "\n",
    "\n",
    "# ====== UI ======\n",
    "with gr.Blocks(title=\"å°½è°ƒé—®ç­”åŠ©æ‰‹\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## å°½è°ƒé—®ç­”åŠ©æ‰‹\\nè¾“å…¥é—®é¢˜ï¼Œæˆ‘ä¼šåŸºäºŽæœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢å¹¶å›žç­”ã€‚\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=420, show_copy_button=True, label=\"å¯¹è¯\")\n",
    "        with gr.Column():\n",
    "            hits = gr.Markdown(label=\"å‘½ä¸­æ–‡æ¡£ï¼ˆTopï¼‰\", value=\"ï¼ˆç­‰å¾…æ£€ç´¢â€¦ï¼‰\")\n",
    "            dbg  = gr.Markdown(label=\"è°ƒè¯•ä¿¡æ¯\", value=\"â€”\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"ä¾‹å¦‚ï¼šå…¬å¸ç®€ä»‹ / é£ŽæŽ§å›¢é˜ŸèŒè´£ / äº¤æ˜“ç³»ç»Ÿæž¶æž„â€¦\", scale=6)\n",
    "        send = gr.Button(\"å‘é€\", variant=\"primary\", scale=1)\n",
    "        clear = gr.Button(\"æ¸…ç©º\", scale=1)\n",
    "\n",
    "    with gr.Accordion(\"é«˜çº§è®¾ç½®\", open=False):\n",
    "        temperature = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label=\"ç”Ÿæˆæ¸©åº¦\")\n",
    "        n_ctx = gr.Slider(3, 30, value=20, step=1, label=\"å¬å›žæ¡æ•°\")\n",
    "        index_name = gr.Textbox(value=DEFAULT_INDEX, label=\"ES ç´¢å¼•å\")\n",
    "        show_full_hits = gr.Checkbox(value=False, label=\"å±•å¼€å‘½ä¸­æ–‡æ¡£å…¨æ–‡\")\n",
    "        snippet_chars = gr.Slider(200, 4000, value=800, step=50, label=\"å‘½ä¸­æ–‡æ¡£æˆªæ–­é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰\")\n",
    "        answer_len = gr.Slider(256, 2048, value=1024, step=64, label=\"ç­”æ¡ˆæœ€å¤§é•¿åº¦ï¼ˆtokensï¼‰\")\n",
    "\n",
    "\n",
    "    def _submit(user_msg, chat_hist, temperature, n_ctx, index_name, show_full_hits, snippet_chars, answer_len):\n",
    "        return respond(user_msg, chat_hist, temperature, int(n_ctx), index_name,\n",
    "                   show_full_hits, snippet_chars, answer_len)\n",
    "\n",
    "    send.click(_submit, [msg, chatbot, temperature, n_ctx, index_name, show_full_hits, snippet_chars, answer_len],\n",
    "                    [chatbot, hits, dbg]).then(lambda: \"\", None, msg)\n",
    "\n",
    "    msg.submit(_submit, [msg, chatbot, temperature, n_ctx, index_name, show_full_hits, snippet_chars, answer_len],\n",
    "                    [chatbot, hits, dbg]).then(lambda: \"\", None, msg)\n",
    "    clear.click(lambda: ([], \"ï¼ˆå·²æ¸…ç©ºï¼‰\", \"â€”\"), None, [chatbot, hits, dbg])\n",
    "\n",
    "# ä½œä¸ºè„šæœ¬å¯åŠ¨\n",
    "if __name__ == \"__main__\":\n",
    "    # å¦‚æžœéœ€è¦å…¬ç½‘è®¿é—®å¯ä¼  share=Trueï¼›æœåŠ¡å™¨çŽ¯å¢ƒå¯æ”¹ server_name=\"0.0.0.0\"\n",
    "    demo.queue().launch(server_name=\"127.0.0.1\", server_port=7862, share=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da9902ee-f202-4dbb-99e3-1eef83eff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "try:\n",
    "    demo.close()      # è‹¥ä½ æœ‰ demo è¿™ä¸ªå˜é‡\n",
    "except NameError:\n",
    "    pass\n",
    "# å…³æŽ‰å½“å‰è¿›ç¨‹é‡Œæ‰€æœ‰è¿˜åœ¨ç›‘å¬çš„ gradio æœåŠ¡\n",
    "try:\n",
    "    gr.close_all()\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397a523-7728-4278-8ead-ffbd2d43d8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
